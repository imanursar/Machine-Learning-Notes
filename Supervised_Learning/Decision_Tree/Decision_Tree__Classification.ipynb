{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You start at the root node (depth 0, at the top): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is, then you move down to the root’s left child node (depth 1, left). In this case, it is a leaf node (i.e., it does not have any children nodes), so it does not ask any questions: you can simply look at the predicted class for that node and the Decision Tree predicts\n",
    "that your flower is an Iris-Setosa (class=setosa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the many qualities of Decision Trees is that they require\n",
    "very little data preparation. In particular, they don’t require feature\n",
    "scaling or centering at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A node’s samples attribute counts how many training instances it applies to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A node’s value attribute tells you how many training instances of each class this node\n",
    "applies to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a node’s gini attribute measures its impurity: a node is “pure” (gini=0) if all training instances it applies to belong to the same class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decision Tree can also estimate the probability that an instance belongs to a partic‐\n",
    "ular class k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn uses the Classification And Regression Tree (CART) algorithm to train Decision Trees (also called “growing” trees). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is really quite simple: the algorithm first splits the training set in two subsets using a single feature k and a threshold tk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It searches for the pair (k, tk) that produces the purest subsets (weighted by their size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it has successfully split the training set in two, it splits the subsets using the\n",
    "same logic, then the sub-subsets and so on, recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot find a split that will reduce impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few other hyperparameters (described in a moment) \n",
    "control additional stopping conditions (min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the CART algorithm is a greedy algorithm: it greedily searches for an optimum split at the top level, then repeats the process at each level. It does not check whether or not the split will\n",
    "lead to the lowest possible impurity several levels down. A greedy\n",
    "algorithm often produces a reasonably good solution, but it is not\n",
    "guaranteed to be the optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Impurity or Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Gini impurity measure is used, but you can select the entropy impurity\n",
    "measure instead by setting the criterion hyperparameter to \"entropy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entropy frequently used as an impurity measure: a set’s\n",
    "entropy is zero when it contains instances of only one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So should you use Gini impurity or entropy? The truth is, most of the time it does not\n",
    "make a big difference: they lead to similar trees. Gini impurity is slightly faster to\n",
    "compute, so it is a good default. However, when they differ, Gini impurity tends to\n",
    "isolate the most frequent class in its own branch of the tree, while entropy tends to\n",
    "produce slightly more balanced trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees make very few assumptions about the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If left unconstrained, the tree structure will adapt itself to the training data, fitting it very\n",
    "closely, and most likely overfitting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nonparametric model, not because it does not have any parameters (it often has a lot) but because the\n",
    "number of parameters is not determined prior to training, so the model structure is\n",
    "free to stick closely to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a parametric model such as a linear model has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\n",
    "during training.  (regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization hyperparameters depend on the algorithm used, but generally you can at least restrict\n",
    "the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\n",
    "max_depth hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing max_depth will regularize the model and thus reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DecisionTreeClassifier class has a few other parameters that similarly restrict\n",
    "the shape of the Decision Tree: min_samples_split (the minimum number of samples a node must have before it can be split), min_samples_leaf (the minimum number of samples a leaf node must have), min_weight_fraction_leaf (same as min_samples_leaf but expressed as a fraction of the total number of weighted instances), max_leaf_nodes (maximum number of leaf nodes), and max_features\n",
    "(maximum number of features that are evaluated for splitting at each node). Increas‐\n",
    "ing min_* hyperparameters or reducing max_* hyperparameters will regularize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other algorithms work by first training the Decision Tree without\n",
    "restrictions, then pruning (deleting) unnecessary nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A node whose children are all leaf nodes is considered unnecessary if the\n",
    "purity improvement it provides is not statistically significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard statistical tests, such as the χ2 test, are used to estimate the\n",
    "probability that the improvement is purely the result of chance. null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this probability, called the pvalue, is higher than a given threshold (typically 5%, controlled by\n",
    "a hyperparameter), then the node is considered unnecessary and its\n",
    "children are deleted. The pruning continues until all unnecessary nodes have been pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the main issue with Decision Trees is that they are very sensitive to\n",
    "small variations in the training data.\n",
    "\n",
    "since the training algorithm used by Scikit-Learn is stochastic6 you may\n",
    "get very different models even on the same training data (unless you set the\n",
    "random_state hyperparameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
