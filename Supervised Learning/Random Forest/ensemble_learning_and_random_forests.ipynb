{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A group of predictors is called an ensemble; thus, this technique is called Ensemble Learning, and an\n",
    "Ensemble Learning algorithm is called an Ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can train a group of Decision Tree classifiers, each on a different\n",
    "random subset of the training set. To make predictions, you just obtain the predic‐\n",
    "tions of all individual trees, then predict the class that gets the most votes. Such an ensemble of Decision Trees is called a Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VotingClassifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple way to create an even better classifier is to aggregate the predictions of\n",
    "each classifier and predict the class that gets the most votes. This majority-vote classi‐\n",
    "fier is called a hard voting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this voting classifier often achieves a higher accuracy than the\n",
    "best classifier in the ensemble. In fact, even if each classifier is a weak learner (mean‐\n",
    "ing it does only slightly better than random guessing), the ensemble can still be a\n",
    "strong learner (achieving high accuracy), provided there are a sufficient number of\n",
    "weak learners and they are sufficiently diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all classifiers are able to estimate class probabilities (i.e., they have a pre\n",
    "dict_proba() method), then you can tell Scikit-Learn to predict the class with the\n",
    "highest class probability, averaged over all the individual classifiers. This is called so\n",
    "voting. It often achieves higher performance than hard voting because it gives more\n",
    "weight to highly confident votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use the same training algorithm for every\n",
    "predictor, but to train them on different random subsets of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sampling is performed with replacement, this method is called bagging1 (short for\n",
    "bootstrap aggregating2). When sampling is performed without replacement, it is called\n",
    "pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both bagging and pasting allow training instances to be sampled sev‐\n",
    "eral times across multiple predictors, but only bagging allows training instances to be\n",
    "sampled several times for the same predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all predictors are trained, the ensemble can make a prediction for a new\n",
    "instance by simply aggregating the predictions of all predictors. The aggregation\n",
    "function is typically the statistical mode (i.e., the most frequent prediction, just like a\n",
    "hard voting classifier) for classification, or the average for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each individual predictor has a higher bias than if it were trained on the original training set, but\n",
    "aggregation reduces both bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is\n",
    "trained on, so bagging ends up with a slightly higher bias than pasting, but this also\n",
    "means that predictors end up being less correlated so the ensemble’s variance is\n",
    "reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that only about 63% of the training instances are sampled on\n",
    "average for each predictor.6 The remaining 37% of the training instances that are not\n",
    "sampled are called out-of-bag (oob) instances. Note that they are not the same 37%\n",
    "for all predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a predictor never sees the oob instances during training, it can be evaluated on\n",
    "these instances, without the need for a separate validation set or cross-validation. You\n",
    "can evaluate the ensemble itself by averaging out the oob evaluations of each predic‐\n",
    "tor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a Random Forest9 is an ensemble of Decision Trees, generally\n",
    "trained via the bagging method (or sometimes pasting), typically with max_samples\n",
    "set to the size of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few exceptions, a RandomForestClassifier has all the hyperparameters of a\n",
    "DecisionTreeClassifier (to control how trees are grown), plus all the hyperpara‐\n",
    "meters of a BaggingClassifier to control the ensemble itself.\n",
    "\n",
    "The Random Forest algorithm introduces extra randomness when growing trees;\n",
    "instead of searching for the very best feature when splitting a node (see Chapter 6), it\n",
    "searches for the best feature among a random subset of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are growing a tree in a Random Forest, at each node only a random subset\n",
    "of the features is considered for splitting (as discussed earlier). It is possible to make\n",
    "trees even more random by also using random thresholds for each feature rather than\n",
    "searching for the best possible thresholds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A forest of such extremely random trees is simply called an Extremely Randomized\n",
    "Trees ensemble12 (or Extra-Trees for short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one\n",
    "of the most time-consuming tasks of growing a tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to tell in advance whether a RandomForestClassifier\n",
    "will perform better or worse than an ExtraTreesClassifier. Gen‐\n",
    "erally, the only way to know is to try both and compare them using\n",
    "cross-validation (and tuning the hyperparameters using grid\n",
    "search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you look at a single Decision Tree, important features are likely to appear\n",
    "closer to the root of the tree, while unimportant features will often appear closer to\n",
    "the leaves (or not at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is therefore possible to get an estimate of a feature’s importance by computing the average depth at which it appears across all trees in the forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting (originally called hypothesis boosting) refers to any Ensemble method that\n",
    "can combine several weak learners into a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost13 (short for Adaptive Boosting) and Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way for a new predictor to correct its predecessor is to pay a bit more attention\n",
    "to the training instances that the predecessor underfitted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in new predictors focusing more and more on the hard cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build an AdaBoost classifier, a first base classifier (such as a Decision\n",
    "Tree) is trained and used to make predictions on the training set. The relative weight\n",
    "of misclassified training instances is then increased. A second classifier is trained\n",
    "using the updated weights and again it makes predictions on the training set, weights\n",
    "are updated, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier gets many instances wrong, so their weights\n",
    "get boosted. The second classifier therefore does a better job on these instances, and\n",
    "so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this sequential learning technique has some\n",
    "similarities with Gradient Descent, except that instead of tweaking a single predictor’s\n",
    "parameters to minimize a cost function, AdaBoost adds predictors to the ensemble,\n",
    "gradually making it better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all predictors are trained, the ensemble makes predictions very much like bag‐\n",
    "ging or pasting, except that predictors have different weights depending on their\n",
    "overall accuracy on the weighted training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one important drawback to this sequential learning techni‐\n",
    "que: it cannot be parallelized (or only partially), since each predic‐\n",
    "tor can only be trained after the previous predictor has been\n",
    "trained and evaluated. As a result, it does not scale as well as bag‐\n",
    "ging or pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your AdaBoost ensemble is overfitting the training set, you can\n",
    "try reducing the number of estimators or more strongly regulariz‐\n",
    "ing the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stacked generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is based on a simple idea: instead of using trivial functions\n",
    "(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\n",
    "why don’t we train a model to perform this aggregation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the bottom three\n",
    "predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor\n",
    "(called a blender, or a meta learner) takes these predictions as inputs and makes the\n",
    "final prediction (3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the training set is split in two subsets. The first subset is used to train the\n",
    "predictors in the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the first layer predictors are used to make predictions on the second (held-out)\n",
    "set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensures that the predictions are “clean,” since the predictors\n",
    "never saw these instances during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each instance in the hold-out set\n",
    "there are three predicted values. We can create a new training set using these predic‐\n",
    "ted values as input features (which makes this new training set three-dimensional),\n",
    "and keeping the target values. The blender is trained on this new training set, so it\n",
    "learns to predict the target value given the first layer’s predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actually possible to train several different blenders this way (e.g., one using Lin‐\n",
    "ear Regression, another using Random Forest Regression, and so on): we get a whole\n",
    "layer of blenders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
