{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree-boosting has the following advantages:\n",
    "\n",
    "- Automatic modeling of non-linearities, discontinuities, and complex high-order interactions\n",
    "- Robust to outliers in and multicollinearity among predictor variables\n",
    "- Scale-invariance to monotone transformations of the predictor variables\n",
    "- Automatic handling of missing values in predictor variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed effects models are a modeling approach for clustered, grouped, longitudinal, or panel data. Among other things, they have the advantage that they allow for more efficient learning of the chosen model for the regression function (e.g. a linear model or a tree ensemble)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed effects models are a modeling approach for clustered, grouped, longitudinal, or panel data. Among other things, they have the advantage that they allow for more efficient learning of the chosen model for the regression function (e.g. a linear model or a tree ensemble)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As outlined in Sigrist (2020), combined gradient tree-boosting and mixed effects models often performs better than (i) plain vanilla gradient boosting, (ii) standard linear mixed effects models, and (iii) alternative approaches for combing machine learning or statistical models with mixed effects models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling grouped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped data (aka clustered data, longitudinal data, panel data) occurs naturally in many applications when there are multiple measurements for different units of a variable of interest . Examples include:\n",
    "\n",
    "- One wants to investigate the impact of some factors (e.g. learning technique, nutrition, sleep, etc.) on studentsâ€™ test scores and every student does several tests. In this case, the units, i.e. the grouping variable, are the students and the variable of interest is the test score.\n",
    "- A company gathers transaction data about its customers. For every, customer there are several transactions. The units are then the customers and the variable of interest can be any attribute of the transactions such as prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, such grouped data can be modeled using four different approaches:\n",
    "\n",
    "- Ignore the grouping structure. This is rarely a good idea since important information is neglected.\n",
    "\n",
    "- Model each group (i.e. each student or each customer) separately. This is also rarely a good idea as the number of measurements per group is often small relative to the number of different groups.\n",
    "\n",
    "- Include the grouping variable (e.g. student or customer ID) in your model of choice and treat it as a categorical variable. While this is a viable approach, it has the following disadvantages. Often, the number of measurements per group (e.g. number of tests per student, number of transactions per customer) is relatively small and the number of different groups is large (e.g. number of students, customers, etc.). In this case, the model needs to learn many parameters (one for every group) based on relatively little data which can make the learning inefficient. Further, for trees, high cardinality categorical variables can be problematic.\n",
    "\n",
    "- Model the grouping variable using so-called random effects in a mixed effects model. This is often a sensible compromise between the approaches 2. and 3. above. In particular, as illustrated below and outlined in Sigrist (2020), this is beneficial compared to the other approaches in the case of tree-boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodological background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the GPBoost algorithm, it is assumed that the response variable y is the sum of a non-linear mean function F(X) and so-called random effects Zb:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "y = F(X) + Zb + e\n",
    "\n",
    "- y the response variable (aka label)\n",
    "- X contains the predictor variables (aka features) and F() is a potentially non-linear function. In linear mixed effects models, this is simply a linear function. In the GPBoost algorithm, this is an ensemble of trees.\n",
    "- Zb are the random effects which are assumed to follow a multivariate normal distribution\n",
    "- e is an error term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained using the GPBoost algorithm, where trainings means learning the (co-)variance parameters (aka hyper-parameters) of the random effects and the regression function F(X) using a tree ensemble. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the GPBoost algorithm is a boosting algorithm that iteratively learns the (co-)variance parameters and adds a tree to the ensemble of trees using a gradient and/or a Newton boosting step. \n",
    "\n",
    "I.e., the main difference to existing boosting algorithms is that, first, it accounts for dependency among the data due to clustering and, second, it learns the hyper-parameters of the random effects. See Sigrist (2020) for more details on the methodology. \n",
    "\n",
    "In the GPBoost library, hyper-parameters parameters can be learned using (accelerated) gradient descent or Fisher scoring, and trees are learned using the LightGBM library. In particular, this means that the full functionality of LightGBM is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpboost as gpb\n",
    "import numpy as np\n",
    "import sklearn.datasets as datasets\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "ntrain = 5000 # number of samples for training\n",
    "n = 2 * ntrain # combined number of training and test data\n",
    "m = 500  # number of categories / levels for grouping variable\n",
    "sigma2_1 = 1  # random effect variance\n",
    "sigma2 = 1 ** 2  # error variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate non-linear mean function\n",
    "np.random.seed(1)\n",
    "X, F = datasets.make_friedman3(n_samples=n)\n",
    "X = pd.DataFrame(X,columns=['variable_1','variable_2','variable_3','variable_4'])\n",
    "F = F * 10**0.5 # with this choice, the fixed-effects regression function has the same variance as the random effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate random effects\n",
    "group_train = np.arange(ntrain)  # grouping variable\n",
    "for i in range(m):\n",
    "    group_train[int(i * ntrain / m):int((i + 1) * ntrain / m)] = i\n",
    "group_test = np.arange(ntrain) # grouping variable for test data. Some existing and some new groups\n",
    "m_test = 2 * m\n",
    "for i in range(m_test):\n",
    "    group_test[int(i * ntrain / m_test):int((i + 1) * ntrain / m_test)] = i\n",
    "group = np.concatenate((group_train,group_test))\n",
    "b = np.sqrt(sigma2_1) * np.random.normal(size=m_test)  # simulate random effects\n",
    "Zb = b[group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything together\n",
    "xi = np.sqrt(sigma2) * np.random.normal(size=n)  # simulate error term\n",
    "y = F + Zb + xi  # observed data\n",
    "# split train and test data\n",
    "y_train = y[0:ntrain]\n",
    "y_test = y[ntrain:n]\n",
    "X_train = X.iloc[0:ntrain,]\n",
    "X_test = X.iloc[ntrain:n,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train GPModel\n",
    "gp_model = gpb.GPModel(group_data=group_train)\n",
    "# create dataset for gpb.train function\n",
    "data_train = gpb.Dataset(X_train, y_train)\n",
    "# specify tree-boosting parameters as a dict\n",
    "params = { 'objective': 'regression_l2', 'learning_rate': 0.1,\n",
    "    'max_depth': 6, 'min_data_in_leaf': 5, 'verbose': 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance parameters in the following order:\n",
      "['Error_term', 'Group_1']\n",
      "[0.9183072 1.013057 ]\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "bst = gpb.train(params=params, train_set=data_train, gp_model=gp_model, num_boost_round=32)\n",
    "gp_model.summary() # estimated covariance parameters\n",
    "# Covariance parameters in the following order:\n",
    "# ['Error_term', 'Group_1']\n",
    "# [0.9183072 1.013057 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2565271489017904"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "pred = bst.predict(data=X_test, group_data_pred=group_test)\n",
    "y_pred = pred['fixed_effect'] + pred['random_effect_mean'] # sum predictions of fixed effect and random effect\n",
    "np.sqrt(np.mean((y_test - y_pred) ** 2)) # root mean square error (RMSE) on test data. Approx. = 1.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's l2: 2.95333\n",
      "[2]\tcv_agg's l2: 2.87809\n",
      "[3]\tcv_agg's l2: 2.80637\n",
      "[4]\tcv_agg's l2: 2.73882\n",
      "[5]\tcv_agg's l2: 2.67551\n",
      "[6]\tcv_agg's l2: 2.61607\n",
      "[7]\tcv_agg's l2: 2.56055\n",
      "[8]\tcv_agg's l2: 2.50864\n",
      "[9]\tcv_agg's l2: 2.46037\n",
      "[10]\tcv_agg's l2: 2.41501\n",
      "[11]\tcv_agg's l2: 2.37346\n",
      "[12]\tcv_agg's l2: 2.3362\n",
      "[13]\tcv_agg's l2: 2.30184\n",
      "[14]\tcv_agg's l2: 2.27357\n",
      "[15]\tcv_agg's l2: 2.24598\n",
      "[16]\tcv_agg's l2: 2.22221\n",
      "[17]\tcv_agg's l2: 2.20134\n",
      "[18]\tcv_agg's l2: 2.18237\n",
      "[19]\tcv_agg's l2: 2.16783\n",
      "[20]\tcv_agg's l2: 2.15479\n",
      "[21]\tcv_agg's l2: 2.14395\n",
      "[22]\tcv_agg's l2: 2.13474\n",
      "[23]\tcv_agg's l2: 2.12721\n",
      "[24]\tcv_agg's l2: 2.12091\n",
      "[25]\tcv_agg's l2: 2.11533\n",
      "[26]\tcv_agg's l2: 2.10954\n",
      "[27]\tcv_agg's l2: 2.10667\n",
      "[28]\tcv_agg's l2: 2.10448\n",
      "[29]\tcv_agg's l2: 2.10367\n",
      "[30]\tcv_agg's l2: 2.10226\n",
      "[31]\tcv_agg's l2: 2.10135\n",
      "[32]\tcv_agg's l2: 2.10021\n",
      "[33]\tcv_agg's l2: 2.099\n",
      "[34]\tcv_agg's l2: 2.09964\n",
      "[35]\tcv_agg's l2: 2.10045\n",
      "[36]\tcv_agg's l2: 2.10051\n",
      "[37]\tcv_agg's l2: 2.1005\n",
      "[38]\tcv_agg's l2: 2.10066\n",
      "Best number of iterations: 32\n"
     ]
    }
   ],
   "source": [
    "# Parameter tuning using cross-validation (only number of boosting iterations)\n",
    "gp_model = gpb.GPModel(group_data=group_train)\n",
    "cvbst = gpb.cv(params=params, train_set=data_train,\n",
    "               gp_model=gp_model, use_gp_model_for_validation=False,\n",
    "               num_boost_round=100, early_stopping_rounds=5,\n",
    "               nfold=4, verbose_eval=True, show_stdv=False, seed=1)\n",
    "best_iter = np.argmin(cvbst['l2-mean'])\n",
    "print(\"Best number of iterations: \" + str(best_iter))\n",
    "# Best number of iterations: 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance and partial dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='Feature importance', ylabel='Features'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAEWCAYAAADGjIh1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAl7klEQVR4nO3de5xVBb338c+XS8IRRA0kQBHNSwgoCqWkDwzHC14yL5FocI6YxqFTSV5PR00xzVAiQaOno1l6kCOEmmBphcbgk8ZRkAEUQ8tGkUgEBQS0HPk9f6w1sN3MZTPMZq+Z+b5fr/1i3dfvN1vnO+uy11ZEYGZmlgWtSl2AmZlZNYeSmZllhkPJzMwyw6FkZmaZ4VAyM7PMcCiZmVlmOJTMmiBJ10j6SanrMGts8ueUrKWRVAl0BT7MmXxYRPx1F7d5SUQ8sWvVNT2SxgOHRMSoUtdiTZ+PlKylOjMiOuS8GhxIjUFSm1Luv6Gaat2WXQ4ls5SkTpLukbRa0ipJN0tqnc77pKTfSVonaa2k6ZL2TudNA3oCj0raJOlqSWWS3sjbfqWkk9Lh8ZIelHS/pI3A6Lr2X0Ot4yXdnw73khSSLpK0UtI7ksZK+rSkpZLWS/phzrqjJT0t6U5JGyT9UdKJOfO7S5oj6W1Jf5L0lbz95tY9FrgGGJH2viRd7iJJL0l6V9Krkv4tZxtlkt6QdIWkNWm/F+XMby9pkqTX0vp+L6l9Ou84Sc+kPS2RVNaAt9oyzKFktt19QBVwCHA0cApwSTpPwPeA7kBv4ABgPEBE/AvwOtuPvm4rcH9nAQ8CewPT69l/IY4FDgVGAJOBa4GTgD7AeZKG5C37KtAZuAF4WNK+6bwHgDfSXocDt+SGVl7d9wC3ADPT3o9Kl1kDfA7YC7gIuF3SMTnb+ATQCegBXAxMlbRPOu/7wADgs8C+wNXAVkk9gF8BN6fTrwQektRlJ35GlnEOJWupHkn/2l4v6RFJXYHTgG9GxOaIWAPcDpwPEBF/ioi5EfH3iHgL+AEwpPbNF+QPEfFIRGwl+eVd6/4LdFNEvB8RvwU2Aw9ExJqIWAX8P5Kgq7YGmBwRH0TETGAFcIakA4ATgP9It1UB/AT4l5rqjoj3aiokIn4VEX+OxHzgt8D/yVnkA+A76f4fAzYBh0tqBXwZGBcRqyLiw4h4JiL+DowCHouIx9J9zwUWAqfvxM/IMs7ng62lOjv3pgRJnwHaAqslVU9uBaxM5+8H3EHyi7VjOu+dXaxhZc7wgXXtv0Bv5gy/V8N4h5zxVfHRu5xeIzky6g68HRHv5s0bWEvdNZJ0GskR2GEkffwTsCxnkXURUZUzviWtrzPQDvhzDZs9EPiipDNzprUF5tVXjzUdDiWzxErg70DnvF+W1b4HBHBkRKyTdDbww5z5+bexbib5RQxAem0o/zRT7jr17b+x9ZCknGDqCcwB/grsK6ljTjD1BFblrJvf60fGJe0BPAT8KzA7Ij6Q9AjJKdD6rAXeBz4JLMmbtxKYFhFf2WEtazZ8+s4MiIjVJKeYJknaS1Kr9OaG6lN0HUlOMa1Pr21clbeJN4GDc8ZfBtpJOkNSW+A6YI9d2H9j2w+4VFJbSV8kuU72WESsBJ4BviepnaQjSa75TK9jW28CvdJTbwAfI+n1LaAqPWo6pZCi0lOZPwV+kN5w0VrSoDTo7gfOlDQsnd4uvWli/51v37LKoWS23b+S/EJdTnJq7kGgWzrvRuAYYAPJxfaH89b9HnBdeo3qyojYAPw7yfWYVSRHTm9Qt7r239j+l+SmiLXAd4HhEbEunXcB0IvkqOkXwA3p9ZvazEr/XSfp+fQI61Lg5yR9fInkKKxQV5Kc6nsOeBu4FWiVBuZZJHf7vUVy5HQV/j3WrPjDs2YtjKTRJB/0PaHUtZjl818YZmaWGQ4lMzPLDJ++MzOzzPCRkpmZZYY/p7SL9t577zjkkENKXUaj2Lx5M3vuuWepy2gUzakXaF79uJds2t29LFq0aG1E7PCIKIfSLuratSsLFy4sdRmNory8nLKyslKX0SiaUy/QvPpxL9m0u3uR9FpN0336zszMMsOhZGZmmeFQMjOzzHAomZlZZjiUzMwsMxxKZmaWGQ4lMzPLDIeSmZllhkPJzMwyw6FkZmaZ4VAyM7PMcCiZmVlmOJTMzCwzHEpmZpYZDiUzM8sMh5KZmWWGQ8nMzDLDoWRmZpnhUDIzs8xwKJmZWWY4lMzMLDMcSmZmlhkOJTMzywyHkpmZZYZDyczMMsOhZGZmmeFQMjOzzHAomZlZZjiUzMwsMxxKZmaWGQ4lMzPLDIeSmZllhkPJzMwyw6FkZmaZ4VAyM7PMcCiZmVlmOJTMzCwzFBGlrqFJ63nwIdHqvCmlLqNRXNGviknL2pS6jEbRnHqB5tWPe8mOyglnbBsuLy+nrKxst+1b0qKIGJg/3UdKZmYt3MqVK7nsssvo3bs3ffr0YcqU5A/tJUuWMGjQIPr168eZZ57Jxo0bAZg+fTr9+/ff9mrVqhUVFRWNUotDycyshWvTpg1f/epXeemll1iwYAFTp05l+fLlXHLJJUyYMIFly5ZxzjnnMHHiRABGjhxJRUUFFRUVTJs2jV69etG/f/9GqSWToSTpMUl717PMplqm3ytpeAP2eZOkpZIqJP1WUved3YaZWVPUrVs3DjvsMAA6duxI7969WbVqFStWrGDw4MEAnHzyyTz00EM7rPvAAw9wwQUXNFotmQolJVpFxOkRsX43735iRBwZEf2BXwLX7+b9m5mVXGVlJYsXL+bYY4+lb9++zJkzB4BZs2axcuXKHZafOXNmo4ZSUa7QSboVeC0ifpSOjwcCGAzsA7QFrouI2ZJ6AY8D84BBwNmS5gMDI2KtpEeAA4B2wJSIuCtnP5OAocA7wPkR8VZeHQOAHwAdgLXA6IhYXVPNEbExZ3TPtN7a+hsDjAHo3LkL1/erKuCnkn1d2ycXbpuD5tQLNK9+3Et2lJeXbxvetGkTjz/+OOPGjeOSSy7h+eefZ+zYsdx8881cddVVHH/88bRq1eoj6yxfvpyIYO3atR+ZviuKcvedpKOByRExJB1fDpwKrI+IjZI6AwuAQ4EDgVeBz0bEgnT5SraH0r4R8bak9sBzwJCIWCcpgFERMV3S9cB+EfF1SfeSHOnMBuYDZ0XEW5JGAMMi4st11P1d4F+BDcDQ/JCrie++y6bm1As0r37cS3bk3n33xBNPMHHiRIYNG8bll1++w7Ivv/wyo0aN4tlnn9027bLLLqNLly5cc801O73v3Xr3XUQsBvaT1F3SUSRHMquBWyQtBZ4AegBd01Veqw6kGlwqaQlJiB1AEmQAW4GZ6fD9wAl56x0O9AXmSqoArgP2r6fuayPiAGA68PVCejUza+oigttuu43evXt/JJDWrFkDwNatW7n55psZO3bstnlbt25l1qxZnH/++Y1aSzEj/kFgOPAJYAYwEugCDIiID9KjoXbpsptr2oCkMuAkYFBEbJFUnrNOvvxDPgEvRsSgBtT+P8CvgBsasK6ZWZPy9NNPM3fuXP72t79tu4vulltu4ZVXXmHq1KkAnHvuuVx00UXb1nnqqafYf//9Ofjggxu3mIgoygvoAzwDvAx0A8YBd6bzhpKESK/09ULeupVAZ+As4NF02qeA94GydDxIriNBchRUve17ScLwY8CfSAINkutYfeqo99Cc4W8ADxbS52GHHRbNxbx580pdQqNpTr1ENK9+3Es27e5egIVRw+/Uoh0pRcSLkjoCqyJitaTpwKOSFgIVwB8L2MyvgbHpKb8VJKfwqm0G+khaRHINaETe/v+R3hp+h6ROJEeFk4EXa9nXBEmHk5wWfA0YW8tyZmZWJEW9QhcR/XKG15LcXVeTvnnr9coZPa2WbXdIB7+dN310znAFyR1/hdT6hUKWMzOz4snU55TMzKxla7r3MjaQpKnA8XmTp0TEz0pRj5mZbdfiQikivlbqGszMrGY+fWdmZpnhUDIzs8xwKJmZWWY4lMzMLDMcSmZmlhkOJTMzywyHkpmZZYZDyczMMsOhZGZmmeFQMjOzzHAomZlZZjiUzMwsMxxKZmaWGQ4lMzPLDIeSmZllhkPJzMwyw6FkZmaZ4VAyM7PMcCiZmVlmOJTMzCwzHEpmZpYZDiUzM8sMh5KZmWWGQ8nMzDLDoWRmZpnhUDIzs8xwKJmZWWY4lMzMLDMcSmZmlhmKiFLX0KT1PPiQaHXelFKX0Siu6FfFpGVtSl1Go2hOvUDz6se9ZEflhDMAWLlyJZ///Od5//33adWqFWPGjGHcuHEsWbKEsWPHsmnTJnr16sX06dPZa6+9mD59OhMnTty2naVLl/L888/Tv3//gvctaVFEDMyf7iMlM7MWrk2bNnz1q1/lpZdeYsGCBUydOpXly5dzySWXMGHCBJYtW8Y555yzLYhGjhxJRUUFFRUVTJs2jV69eu1UINUlk6Ek6TFJe9ezzKZapt8raXgD9jlR0h8lLZX0i/r2b2bWXHTr1o3DDjsMgI4dO9K7d29WrVrFihUrGDx4MAAnn3wyDz300A7rPvDAA1xwwQWNVkumQkmJVhFxekSs3827nwv0jYgjgZeB/9zN+zczK7nKykoWL17MscceS9++fZkzZw4As2bNYuXKlTssP3PmzEYNpaJcU5J0K/BaRPwoHR8PBDAY2AdoC1wXEbMl9QIeB+YBg4CzgfnAwIhYK+kR4ACgHTAlIu5Kt7kJ+C9gKPAOcH5EvCXpXuCXEfGgpAHAD4AOwFpgdESsLqD+c4DhETGylvljgDEAnTt3GXD95Lt36ueTVV3bw5vvlbqKxtGceoHm1Y97yY5+PTptG960aROtW7dm3LhxjBo1isGDB/P6669z5513smHDBo4//ngefvhhZs+evW2d5cuX8/3vf5+f/vSnO73voUOH1nhNqVihdDQwOSKGpOPLgVOB9RGxUVJnYAFwKHAg8Crw2YhYkC5fyfZQ2jci3pbUHngOGBIR6yQFMCoipku6HtgvIr5eHUrAbJJwOysNqxHAsIj4cgH1PwrMjIj761vWNzpkU3PqBZpXP+4lO6pvdAB44oknmDhxIsOGDePyyy/fYdmXX36ZUaNG8eyzz26bdtlll9GlSxeuueaand53bTc6FOWnGRGLJe0nqTvQheRIZjVwu6TBwFagB9A1XeW16kCqwaXpkQskR0yHAuvSbcxMp98PPJy33uFAX2CuJIDWaQ11knQtUAVMr29ZM7PmICK47bbbOOKIIz4SSGvWrGG//fZj69at3HzzzYwdO3bbvK1btzJr1iyeeuqpRq2loFCS9EngjYj4u6Qy4Ejgv+u57vMgMBz4BDADGEkSUAMi4oP0aKhduuzmWvZbBpwEDIqILZLKc9bJl3/IJ+DFiBhUR435+7sQ+BxwYvheeTNrIZ5++mnmzp3L3/72t2130d1yyy288sorTJ06FYBzzz2Xiy66aNs6Tz31FPvvvz8HH3xwo9ZS6JHSQ8BASYcA9wBzgP8BTq9jnRnA3UBnYAhwHrAmDaShJKft6tMJeCcNpE8Bx+XMa0USejOALwG/z1t3BdBF0qCI+IOktsBhEfFiTTuSdCrwHySnB7cUUJuZWbNwwgknMG/ePMrKynaYN27cuBrXKSsrY8GC2k5wNVyhobQ1IqrS02iTI+JOSYvrWiEiXpTUEVgVEaslTQcelbQQqAD+WMB+fw2MlbSUJGRyfwKbgT6SFgEbgBF5+/9Hemv4HZI6kfQ6GagxlIAfAnuw/XTfgogYW8uy27Rv25oVOedlm7Ly8nIqR5aVuoxG0Zx6gebVj3uxuhQaSh9IugC4EDgznda2vpUiol/O8FqSu+tq0jdvvV45o6fVsu0O6eC386aPzhmuILnjr14RcUghy5mZWfEU+jmli0gC5bsR8RdJB5HcXGBmZtZoCjpSiojlkv4D6JmO/wWYUMzCikXSVOD4vMlTIuJnpajHzMy2K/TuuzOB7wMfAw6S1B/4TkR8voi1FUVEfK3UNZiZWc0KPX03HvgMsB62Xas5qCgVmZlZi1VoKFVFxIa8af4cj5mZNapC7757QdKXgNaSDgUuBZ4pXllmZtYSFXqk9A2gD/B3kg/NbgC+WaSazMyshar3SElSa2BORJwEXFv8kszMrKWq90gpIj4EtqRPRTAzMyuaQq8pvQ8skzSXnIenRsSlRanKzMxapEJD6Vfpy8zMrGgKfaLDfcUuxMzMrNAnOvyFGj6XFBGN+0UaZmbWohV6+i73K2vbAV8E9m38cszMrCUr6HNKEbEu57UqIiYD/1zc0szMrKUp9PTdMTmjrUiOnDoWpSIzM2uxCj19NylnuAr4C8nXm5uZmTWaQkPp4oh4NXdC+kV/ZmZmjabQZ989WOA0MzOzBqvzSEnSp0gexNpJ0rk5s/YiuQvPzMys0dR3+u5w4HPA3sCZOdPfBb5SpJrMzKyFqjOUImI2MFvSoIj4w26qyczMWqhCb3RYLOlrJKfytp22i4gvF6UqMzNrkQq90WEa8AlgGDAf2J/kFJ6ZmVmjKTSUDomIbwOb04ezngH0K15ZZmbWEhUaSh+k/66X1BfoBPQqSkVmZtZiFXpN6S5J+wDfBuYAHYDri1aVmZm1SIV+n9JP0sH5gL+uwszMiqKg03eSukq6R9Lj6fgRki4ubmlmZtbSFHpN6V7gN0D3dPxl4JtFqMfMzFqwQkOpc0T8HNgKEBFVwIdFq8rMzFqkQkNps6SPk34luqTjgA1Fq8rMzFokRUT9CyVf8ncn0Bd4AegCDI+IpcUtL/t6HnxItDpvSqnLaBRX9Kti0rJCb8jMtubUCzSvftxLcVVOOKNB65WXl1NWVta4xdRB0qKIGJg/vc4jJUk9ASLieWAI8Fng34A+DiQzs+xauXIlQ4cOpXfv3vTp04cpU5I/nisqKjjuuOPo378/AwcO5Nlnn922ztKlSxk0aBB9+vShX79+vP/++7u97voi/hGg+qvQZ0bEF4pbjpmZNYY2bdowadIkjjnmGN59910GDBjAySefzNVXX80NN9zAaaedxmOPPcbVV19NeXk5H374IaNGjWLatGkcddRRrFu3jrZt2+72uuu7pqSc4d32+SRJj0nau55lNtUy/V5Jwxuwzy9KelHSVkk7HFKamTUl3bp145hjkmOKjh070rt3b1atWoUkNm7cCMCGDRvo3j25qfq5557jyCOP5KijjgLg4x//OK1bt97tddd3pBS1DBeFJJFc5zq92PuqwQvAucB/lWDfZmZFU1lZyeLFizn22GOZPHkyw4YN48orr2Tr1q0888wzALzxxhtIYtiwYbz11lucf/75XH311bu91vpC6ShJG0mOmNqnw6TjERF71bSSpFuB1yLiR+n4eJJQGwzsA7QFrouI2ZJ6AY8D84BBwNmS5gMDI2KtpEeAA0i+MmNKRNyVs59JwFDgHeD8iHgrr44BwA9IHou0FhgdEatrqjkiXkrXqedHApLGAGMAOnfuwvX9qupdpyno2j65cNscNKdeoHn1416Kq7y8/CPj7733HuPGjeOSSy7h+eef54477uDiiy9myJAhzJs3j3PPPZdJkyaxZcsWnnjiCX784x+zxx57cMUVV9C6dWsGDBiwW+sv6O67nd6odDQwOSKGpOPLgVOB9RGxUVJnYAFwKHAg8Crw2YhYkC5fyfZQ2jci3pbUHngOGBIR6yQFMCoipku6HtgvIr4u6V7gl8BskscinRURb0kaAQyr7zugJJUDV0bEwkJ69d132dSceoHm1Y97Ka7cu+8++OADPve5zzFs2DAuv/xyADp16sT69euRRETQqVMnNm7cyPXXX8/rr7/OvffeC8BNN91Eu3btuOqqq4pSZ4PuvmuoiFgM7Cepu6SjSI5kVgO3SFoKPAH0ALqmq7xWHUg1uFTSEpIQO4AkyCD5IO/MdPh+4IS89Q4nuYV9rqQK4DqS74EyM2v2IoKLL76Y3r17bwskgO7duzN//nwAfve733Hoocmv1E9/+tMsXbqULVu2UFVVxfz58zniiCN2e93FjPgHgeEkXw44AxhJ8vmmARHxQXo0VP0ttptr2oCkMuAkYFBEbEmPYtrVtCw7XvMS8GJEDGp4C2ZmTdPTTz/NtGnT6NevH/379wfglltu4e6772bcuHFUVVXRrl077roruSLSsWNHLr/8cj796U8jidNPP50zzmjYZ552RTFDaQZwN9CZ5DNO5wFr0kAaSnLarj6dgHfSQPoUcFzOvFYkoTcD+BLw+7x1VwBdJA2KiD9IagscFhEv7lJXedq3bc2KBn5YLWvKy8upHFlW6jIaRXPqBZpXP+5l9zjhhBOo7fLMokWLapw+atQoRo0aVcyy6lWU03cA6S//jsCq9OaC6cBASQtJjpr+WMBmfg20SU/53URyCq/aZqCPpEXAPwPfydv/P0hC69b09F8FyYd/ayTpHElvkNxs8StJvymoUTMzazRFvUIXEf1yhteS/MKvSd+89XrljJ5Wy7Y7pIPfzps+Ome4guSOv0Jq/QXwi0KWNTOz4ijakZKZmdnOyta9jLuBpKnA8XmTp0TEz0pRj5mZbdfiQikivlbqGszMrGY+fWdmZpnhUDIzs8xwKJmZWWY4lMzMLDMcSmZmlhkOJTMzywyHkpmZZYZDyczMMsOhZGZmmeFQMjOzzHAomZlZZjiUzMwsMxxKZmaWGQ4lMzPLDIeSmZllhkPJzMwyw6FkZmaZ4VAyM7PMcCiZmVlmOJTMzCwzHEpmZpYZDiUzM8sMh5KZmWWGQ8nMzDLDoWRmZpnhUDIzs8xwKJmZWWY4lMzMLDMcSmZmlhltSl1AU/feBx/S61u/KnUZjeKKflWMLlEvlRPO2DZ8++2385Of/ARJ9OvXj5/97Gds2bKFESNGUFlZSa9evfj5z3/OPvvsU5Jazax4fKRkmbJq1SruuOMOFi5cyAsvvMCHH37IjBkzmDBhAieeeCKvvPIKJ554IhMmTCh1qWZWBJkMJUmPSdq7nmU21TL9XknDd2HfV0oKSZ0bug3bNVVVVbz33ntUVVWxZcsWunfvzuzZs7nwwgsBuPDCC3nkkUdKW6SZFUWmQkmJVhFxekSsL8H+DwBOBl7f3fu2RI8ePbjyyivp2bMn3bp1o1OnTpxyyim8+eabdOvWDYBu3bqxZs2aEldqZsVQlGtKkm4FXouIH6Xj44EABgP7AG2B6yJitqRewOPAPGAQcLak+cDAiFgr6RHgAKAdMCUi7srZzyRgKPAOcH5EvJVXxwDgB0AHYC0wOiJW11H67cDVwOx6+hsDjAHo3LkL1/erqu9H0iR0bZ9cVyqF8vJyAN59913uu+8+7r//fjp06MD48eO59tprqaqq2rYMsMN4vk2bNtU5v6lpTv24l2zKSi/FutFhBjAZ+FE6fh5wKnB7RGxMT40tkDQnnX84cFFE/DuApNxtfTki3pbUHnhO0kMRsQ7YE3g+Iq6QdD1wA/D16pUktQXuBM6KiLckjQC+C3y5poIlfR5YFRFL8va/gzQY7wLoefAhMWlZ87hf5Ip+VZSql8qRZQDMmjWLo48+mrPPPhuAv/71ryxYsIAePXpw+OGH061bN1avXk337t0pKyurdXvl5eV1zm9qmlM/7iWbstJLUX4DRcRiSftJ6g50ITmSWQ3cLmkwsBXoAXRNV3ktIhbUsrlLJZ2TDh8AHAqsS7cxM51+P/Bw3nqHA32BuWnItE5r2IGkfwKuBU7ZmT6t8fXs2ZMFCxawZcsW2rdvz5NPPsnAgQPZc889ue+++/jWt77Ffffdx1lnnVXqUs2sCIr5Z/GDwHDgEyRHTiNJAmpARHwgqZLklBzA5po2IKkMOAkYFBFbJJXnrJMv8lcHXoyIQQXU+kngIKD6KGl/4HlJn4mIvxWwvjWSY489luHDh3PMMcfQpk0bjj76aMaMGcOmTZs477zzuOeee+jZsyezZs0qdalmVgTFDKUZwN1AZ2AIySm8NWkgDQUOLGAbnYB30kD6FHBczrxWJKE3A/gS8Pu8dVcAXSQNiog/pKfzDouIF/N3EhHLgP2qx9PAHBgRawtr1RrTjTfeyI033viRaXvssQdPPvlkiSoys92laKEUES9K6khynWa1pOnAo5IWAhXAHwvYzK+BsZKWkoRM7im+zUAfSYuADcCIvP3/I701/A5JnUh6nQzsEEq7on3b1qzI+eBnU1ZeXr7t2o6ZWSkU9ap2RPTLGV5LcnddTfrmrdcrZ/S0WrbdIR38dt700TnDFSR3/O2UvP2bmdlukqnPKZmZWcvWPO5l3gmSpgLH502eEhE/K0U9Zma2XYsLpYj4WqlrMDOzmvn0nZmZZYZDyczMMsOhZGZmmeFQMjOzzHAomZlZZjiUzMwsMxxKZmaWGQ4lMzPLDIeSmZllhkPJzMwyw6FkZmaZ4VAyM7PMcCiZmVlmOJTMzCwzHEpmZpYZDiUzM8sMh5KZmWWGQ8nMzDLDoWRmZpnhUDIzs8xwKJmZWWY4lMzMLDMcSmZmlhkOJTMzywyHkpmZZYZDyczMMsOhZGZmmeFQMjOzzHAomZlZZjiUzMwsMxxKZmaWGQ4lMzPLDIeSmZllhkPJzMwyQxFR6hqaNEnvAitKXUcj6QysLXURjaQ59QLNqx/3kk27u5cDI6JL/sQ2u7GA5mpFRAwsdRGNQdJC95JNzakf95JNWenFp+/MzCwzHEpmZpYZDqVdd1epC2hE7iW7mlM/7iWbMtGLb3QwM7PM8JGSmZllhkPJzMwyw6HUQJJOlbRC0p8kfavU9TSEpEpJyyRVSFqYTttX0lxJr6T/7lPqOmsi6aeS1kh6IWdarbVL+s/0vVohaVhpqq5ZLb2Ml7QqfW8qJJ2eMy/LvRwgaZ6klyS9KGlcOr3JvTd19NJU35t2kp6VtCTt58Z0erbem4jwaydfQGvgz8DBwMeAJcARpa6rAX1UAp3zpt0GfCsd/hZwa6nrrKX2wcAxwAv11Q4ckb5HewAHpe9d61L3UE8v44Era1g26710A45JhzsCL6c1N7n3po5emup7I6BDOtwW+F/guKy9Nz5SapjPAH+KiFcj4h/ADOCsEtfUWM4C7kuH7wPOLl0ptYuIp4C38ybXVvtZwIyI+HtE/AX4E8l7mAm19FKbrPeyOiKeT4ffBV4CetAE35s6eqlNZnsBiMSmdLRt+goy9t44lBqmB7AyZ/wN6v6PNasC+K2kRZLGpNO6RsRqSP6nBPYrWXU7r7bam+r79XVJS9PTe9WnVJpML5J6AUeT/EXepN+bvF6gib43klpLqgDWAHMjInPvjUOpYVTDtKZ4b/3xEXEMcBrwNUmDS11QkTTF9+v/Ap8E+gOrgUnp9CbRi6QOwEPANyNiY12L1jAtU/3U0EuTfW8i4sOI6A/sD3xGUt86Fi9JPw6lhnkDOCBnfH/gryWqpcEi4q/pv2uAX5Acmr8pqRtA+u+a0lW402qrvcm9XxHxZvoLZCtwN9tPm2S+F0ltSX6JT4+Ih9PJTfK9qamXpvzeVIuI9UA5cCoZe28cSg3zHHCopIMkfQw4H5hT4pp2iqQ9JXWsHgZOAV4g6ePCdLELgdmlqbBBaqt9DnC+pD0kHQQcCjxbgvoKVv1LInUOyXsDGe9FkoB7gJci4gc5s5rce1NbL034vekiae90uD1wEvBHsvbelPqOkKb6Ak4nuRvnz8C1pa6nAfUfTHJnzRLgxeoegI8DTwKvpP/uW+paa6n/AZJTJx+Q/EV3cV21A9em79UK4LRS119AL9OAZcBSkl8O3ZpILyeQnOJZClSkr9Ob4ntTRy9N9b05Elic1v0CcH06PVPvjR8zZGZmmeHTd2ZmlhkOJTMzywyHkpmZZYZDyczMMsOhZGZmmeFQMquFpA9zngRdkT5qZme3cbakI4pQHpK6S3qwGNuuY5/9c5+KbdbY2pS6ALMMey+SR7LsirOBXwLLC11BUpuIqKpvuUieyDG84aXtHEltSB6tMxB4bHft11oWHymZ7QRJAyTNTx9i+5ucx7N8RdJz6XfVPCTpnyR9Fvg8MDE90vqkpHJJA9N1OkuqTIdHS5ol6VGSh+TumT7s8zlJiyXt8BR6Sb2UfgdTuv4jkh6V9BdJX5d0ebruAkn7psuVS5os6RlJL0j6TDp933T9penyR6bTx0u6S9Jvgf8GvgOMSPsZIekz6bYWp/8enlPPw5J+reR7em7LqftUSc+nP6sn02n19mstRKk/ZeyXX1l9AR+y/ZP8vyB51P8zQJd0/gjgp+nwx3PWuxn4Rjp8LzA8Z145MDAd7gxUpsOjSZ7msG86fgswKh3em+TpIXvm1deL9DuY0vX/RPK9P12ADcDYdN7tJA8Trd7/3enw4Jz17wRuSIf/GahIh8cDi4D2Ofv5YU4NewFt0uGTgIdylnsV6AS0A14jeY5aF5InTx+ULldwv361jJdP35nV7iOn79InKvcF5iaPRaM1yeOBAPpKupnkF2oH4DcN2N/ciKj+XqVTgM9LujIdbwf0JPlOn9rMi+R7f96VtAF4NJ2+jOQRM9UegOR7nCTtlT4P7QTgC+n030n6uKRO6fJzIuK9WvbZCbhP0qEkj+RpmzPvyYjYACBpOXAgsA/wVCTfz8Mu9mvNkEPJrHACXoyIQTXMuxc4OyKWSBoNlNWyjSq2nzZvlzdvc96+vhARK3aivr/nDG/NGd/KR/9fz3+2WFD31xRsrmFetZtIwvCc9EaQ8lrq+TCtQTXsHxrWrzVDvqZkVrgVQBdJgyD5WgNJfdJ5HYHVSr7qYGTOOu+m86pVAgPS4bpuUvgN8I30SdVIOnrXy99mRLrNE4AN6dHMU6R1SyoD1kbN34OU308nYFU6PLqAff8BGJI+dZrqa10Ut19rQhxKZgWKiH+QBMmtkpaQXGv6bDr72yTfSjqX5OsAqs0Arkov3n8S+D7wVUnPkFxTqs1NJKfClqY3M9zUiK28k+7/xyRPJIfk2tFASUuBCWz/KoN884Ajqm90AG4DvifpaZLTmXWKiLeAMcDD6c9wZjqrmP1aE+KnhJu1IJLKgSsjYmGpazGriY+UzMwsM3ykZGZmmeEjJTMzywyHkpmZZYZDyczMMsOhZGZmmeFQMjOzzPj/wTkMlm5PhWEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting feature importances\n",
    "gpb.plot_importance(bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "group_data_pred not provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e9a6f01368e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m pdp_dist = pdp.pdp_isolate(model=bst, dataset=X_train,\n\u001b[0;32m      4\u001b[0m                            \u001b[0mmodel_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'variable_2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                            num_grid_points=100)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdp_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdp_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'variable_2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\pdpbox\\pdp.py\u001b[0m in \u001b[0;36mpdp_isolate\u001b[1;34m(model, dataset, model_features, feature, num_grid_points, grid_type, percentile_range, grid_range, cust_grid_points, memory_limit, n_jobs, predict_kwds, data_transformer)\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mfeature_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             feature=feature, feature_type=feature_type, predict_kwds=predict_kwds, data_transformer=data_transformer)\n\u001b[1;32m--> 159\u001b[1;33m         for feature_grid in feature_grids)\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\pdpbox\\pdp_calc_utils.py\u001b[0m in \u001b[0;36m_calc_ice_lines\u001b[1;34m(feature_grid, data, model, model_features, n_classes, feature, feature_type, predict_kwds, data_transformer, unit_test)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# get predictions for this chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\gpboost\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, group_data_pred, group_rand_coef_data_pred, gp_coords_pred, gp_rand_coef_data_pred, cluster_ids_pred, vecchia_pred_type, num_neighbors_pred, predict_cov_mat, **kwargs)\u001b[0m\n\u001b[0;32m   2495\u001b[0m                                                        \u001b[0mvecchia_pred_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvecchia_pred_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2496\u001b[0m                                                        \u001b[0mnum_neighbors_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_neighbors_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2497\u001b[1;33m                                                        predict_cov_mat=predict_cov_mat)\n\u001b[0m\u001b[0;32m   2498\u001b[0m             fixed_effect = predictor.predict(data, num_iteration,\n\u001b[0;32m   2499\u001b[0m                                               \u001b[0mraw_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\gpboost\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, y, group_data_pred, group_rand_coef_data_pred, gp_coords_pred, gp_rand_coef_data_pred, vecchia_pred_type, num_neighbors_pred, cluster_ids_pred, predict_cov_mat, cov_pars, X_pred, use_saved_data)\u001b[0m\n\u001b[0;32m   3598\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_group_re\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3599\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgroup_data_pred\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3600\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"group_data_pred not provided\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3601\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_data_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3602\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"group_data_pred needs to be a numpy.ndarray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: group_data_pred not provided"
     ]
    }
   ],
   "source": [
    "from pdpbox import pdp\n",
    "# Single variable plots\n",
    "pdp_dist = pdp.pdp_isolate(model=bst, dataset=X_train,\n",
    "                           model_features=X_train.columns,feature='variable_2',\n",
    "                           num_grid_points=100)\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, 'variable_2', plot_lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate partial dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "group_data_pred not provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b4812e2d10f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Two variable interaction plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m inter_rf = pdp.pdp_interact(model=bst, dataset=X_train, model_features=X_train.columns,\n\u001b[1;32m----> 3\u001b[1;33m                              features=['variable_1','variable_2'])\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdp_interact_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minter_rf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'variable_1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'variable_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_quantile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'contour'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_pdp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\pdpbox\\pdp.py\u001b[0m in \u001b[0;36mpdp_interact\u001b[1;34m(model, dataset, model_features, features, num_grid_points, grid_types, percentile_ranges, grid_ranges, cust_grid_points, memory_limit, n_jobs, predict_kwds, data_transformer)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0mnum_grid_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_grid_points\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid_types\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercentile_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpercentile_ranges\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[0mgrid_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid_ranges\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcust_grid_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcust_grid_points\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemory_limit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m             n_jobs=n_jobs, predict_kwds=predict_kwds, data_transformer=data_transformer)\n\u001b[0m\u001b[0;32m    559\u001b[0m         \u001b[0mpdp_isolate_outs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdp_isolate_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\pdpbox\\pdp.py\u001b[0m in \u001b[0;36mpdp_isolate\u001b[1;34m(model, dataset, model_features, feature, num_grid_points, grid_type, percentile_range, grid_range, cust_grid_points, memory_limit, n_jobs, predict_kwds, data_transformer)\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mfeature_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             feature=feature, feature_type=feature_type, predict_kwds=predict_kwds, data_transformer=data_transformer)\n\u001b[1;32m--> 159\u001b[1;33m         for feature_grid in feature_grids)\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\pdpbox\\pdp_calc_utils.py\u001b[0m in \u001b[0;36m_calc_ice_lines\u001b[1;34m(feature_grid, data, model, model_features, n_classes, feature, feature_type, predict_kwds, data_transformer, unit_test)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# get predictions for this chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\gpboost\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, group_data_pred, group_rand_coef_data_pred, gp_coords_pred, gp_rand_coef_data_pred, cluster_ids_pred, vecchia_pred_type, num_neighbors_pred, predict_cov_mat, **kwargs)\u001b[0m\n\u001b[0;32m   2495\u001b[0m                                                        \u001b[0mvecchia_pred_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvecchia_pred_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2496\u001b[0m                                                        \u001b[0mnum_neighbors_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_neighbors_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2497\u001b[1;33m                                                        predict_cov_mat=predict_cov_mat)\n\u001b[0m\u001b[0;32m   2498\u001b[0m             fixed_effect = predictor.predict(data, num_iteration,\n\u001b[0;32m   2499\u001b[0m                                               \u001b[0mraw_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\python\\lib\\site-packages\\gpboost\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, y, group_data_pred, group_rand_coef_data_pred, gp_coords_pred, gp_rand_coef_data_pred, vecchia_pred_type, num_neighbors_pred, cluster_ids_pred, predict_cov_mat, cov_pars, X_pred, use_saved_data)\u001b[0m\n\u001b[0;32m   3598\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_group_re\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3599\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgroup_data_pred\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3600\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"group_data_pred not provided\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3601\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_data_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3602\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"group_data_pred needs to be a numpy.ndarray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: group_data_pred not provided"
     ]
    }
   ],
   "source": [
    "# Two variable interaction plot\n",
    "inter_rf = pdp.pdp_interact(model=bst, dataset=X_train, model_features=X_train.columns,\n",
    "                             features=['variable_1','variable_2'])\n",
    "pdp.pdp_interact_plot(inter_rf, ['variable_1','variable_2'], x_quantile=True, plot_type='contour', plot_pdp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9c3f269a05b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTreeExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdependence_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"variable_2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshap_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "shap_values = shap.TreeExplainer(bst).shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "shap.dependence_plot(\"variable_2\", shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to alternative approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare the GPBoost algorithm to several existing approaches using the above simulated data. We consider the following alternative approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A linear mixed effects model (â€˜Linear_MEâ€™) where F(X) is a linear function\n",
    "- Standard gradient tree-boosting ignoring the grouping structure (â€˜Boosting_Ignâ€™)\n",
    "- Standard gradient tree-boosting including the grouping variable as a categorical variables (â€˜Boosting_Catâ€™)\n",
    "- Mixed-effects random forest (â€˜MERFâ€™) (see here and Hajjem et al. (2014) for more information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the algorithms in terms of predictive accuracy measured using the root mean square error (RMSE) and computational time (clock time in seconds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's l2: 2.85986\n",
      "[2]\tcv_agg's l2: 2.71862\n",
      "[3]\tcv_agg's l2: 2.60302\n",
      "[4]\tcv_agg's l2: 2.5079\n",
      "[5]\tcv_agg's l2: 2.43235\n",
      "[6]\tcv_agg's l2: 2.3709\n",
      "[7]\tcv_agg's l2: 2.31853\n",
      "[8]\tcv_agg's l2: 2.27736\n",
      "[9]\tcv_agg's l2: 2.24275\n",
      "[10]\tcv_agg's l2: 2.21561\n",
      "[11]\tcv_agg's l2: 2.19099\n",
      "[12]\tcv_agg's l2: 2.17319\n",
      "[13]\tcv_agg's l2: 2.16091\n",
      "[14]\tcv_agg's l2: 2.14851\n",
      "[15]\tcv_agg's l2: 2.14156\n",
      "[16]\tcv_agg's l2: 2.13417\n",
      "[17]\tcv_agg's l2: 2.13021\n",
      "[18]\tcv_agg's l2: 2.12591\n",
      "[19]\tcv_agg's l2: 2.12284\n",
      "[20]\tcv_agg's l2: 2.11912\n",
      "[21]\tcv_agg's l2: 2.11918\n",
      "[22]\tcv_agg's l2: 2.11704\n",
      "[23]\tcv_agg's l2: 2.11802\n",
      "[24]\tcv_agg's l2: 2.11885\n",
      "[25]\tcv_agg's l2: 2.1196\n",
      "[26]\tcv_agg's l2: 2.11938\n",
      "[27]\tcv_agg's l2: 2.12028\n",
      "Best number of iterations: 21\n",
      "[1]\tcv_agg's l2: 2.85986\n",
      "[2]\tcv_agg's l2: 2.71862\n",
      "[3]\tcv_agg's l2: 2.60302\n",
      "[4]\tcv_agg's l2: 2.5079\n",
      "[5]\tcv_agg's l2: 2.43235\n",
      "[6]\tcv_agg's l2: 2.3709\n",
      "[7]\tcv_agg's l2: 2.31853\n",
      "[8]\tcv_agg's l2: 2.27736\n",
      "[9]\tcv_agg's l2: 2.24275\n",
      "[10]\tcv_agg's l2: 2.21541\n",
      "[11]\tcv_agg's l2: 2.19103\n",
      "[12]\tcv_agg's l2: 2.17415\n",
      "[13]\tcv_agg's l2: 2.16129\n",
      "[14]\tcv_agg's l2: 2.14914\n",
      "[15]\tcv_agg's l2: 2.14097\n",
      "[16]\tcv_agg's l2: 2.13479\n",
      "[17]\tcv_agg's l2: 2.12859\n",
      "[18]\tcv_agg's l2: 2.12535\n",
      "[19]\tcv_agg's l2: 2.1214\n",
      "[20]\tcv_agg's l2: 2.11931\n",
      "[21]\tcv_agg's l2: 2.11784\n",
      "[22]\tcv_agg's l2: 2.11678\n",
      "[23]\tcv_agg's l2: 2.11457\n",
      "[24]\tcv_agg's l2: 2.11358\n",
      "[25]\tcv_agg's l2: 2.11224\n",
      "[26]\tcv_agg's l2: 2.11131\n",
      "[27]\tcv_agg's l2: 2.11074\n",
      "[28]\tcv_agg's l2: 2.11064\n",
      "[29]\tcv_agg's l2: 2.11113\n",
      "[30]\tcv_agg's l2: 2.11175\n",
      "[31]\tcv_agg's l2: 2.11253\n",
      "[32]\tcv_agg's l2: 2.11429\n",
      "[33]\tcv_agg's l2: 2.11513\n",
      "Best number of iterations: 27\n",
      "Warning: the following takes a lot of time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [utils.py:141] NumExpr defaulting to 8 threads.\n",
      "INFO     [merf.py:307] Training GLL is -4351.463311561931 at iteration 1.\n",
      "INFO     [merf.py:307] Training GLL is -4524.798637272867 at iteration 2.\n",
      "INFO     [merf.py:307] Training GLL is -4530.719728338003 at iteration 3.\n",
      "INFO     [merf.py:307] Training GLL is -4488.993561732143 at iteration 4.\n",
      "INFO     [merf.py:307] Training GLL is -4427.488904860089 at iteration 5.\n",
      "INFO     [merf.py:307] Training GLL is -4438.485716175571 at iteration 6.\n",
      "INFO     [merf.py:307] Training GLL is -4389.939419100021 at iteration 7.\n",
      "INFO     [merf.py:307] Training GLL is -4383.770291783691 at iteration 8.\n",
      "INFO     [merf.py:307] Training GLL is -4380.408400251073 at iteration 9.\n",
      "INFO     [merf.py:307] Training GLL is -4348.7528211290355 at iteration 10.\n",
      "INFO     [merf.py:307] Training GLL is -4368.453675183156 at iteration 11.\n",
      "INFO     [merf.py:307] Training GLL is -4357.52476912367 at iteration 12.\n",
      "INFO     [merf.py:307] Training GLL is -4372.794444972436 at iteration 13.\n",
      "INFO     [merf.py:307] Training GLL is -4378.693663803477 at iteration 14.\n",
      "INFO     [merf.py:307] Training GLL is -4313.45534219158 at iteration 15.\n",
      "INFO     [merf.py:307] Training GLL is -4384.51388064513 at iteration 16.\n",
      "INFO     [merf.py:307] Training GLL is -4372.970905065818 at iteration 17.\n",
      "INFO     [merf.py:307] Training GLL is -4349.6870276784 at iteration 18.\n",
      "INFO     [merf.py:307] Training GLL is -4329.823067670301 at iteration 19.\n",
      "INFO     [merf.py:307] Training GLL is -4343.517941989715 at iteration 20.\n",
      "INFO     [merf.py:307] Training GLL is -4349.181732821227 at iteration 21.\n",
      "INFO     [merf.py:307] Training GLL is -4334.650689579558 at iteration 22.\n",
      "INFO     [merf.py:307] Training GLL is -4344.3226510312925 at iteration 23.\n",
      "INFO     [merf.py:307] Training GLL is -4365.129169580226 at iteration 24.\n",
      "INFO     [merf.py:307] Training GLL is -4328.853460236798 at iteration 25.\n",
      "INFO     [merf.py:307] Training GLL is -4320.526263411404 at iteration 26.\n",
      "INFO     [merf.py:307] Training GLL is -4360.66799264162 at iteration 27.\n",
      "INFO     [merf.py:307] Training GLL is -4337.990053699439 at iteration 28.\n",
      "INFO     [merf.py:307] Training GLL is -4360.121959937924 at iteration 29.\n",
      "INFO     [merf.py:307] Training GLL is -4308.660599168728 at iteration 30.\n",
      "INFO     [merf.py:307] Training GLL is -4350.488909345554 at iteration 31.\n",
      "INFO     [merf.py:307] Training GLL is -4346.044529477831 at iteration 32.\n",
      "INFO     [merf.py:307] Training GLL is -4346.357848024645 at iteration 33.\n",
      "INFO     [merf.py:307] Training GLL is -4360.125893136671 at iteration 34.\n",
      "INFO     [merf.py:307] Training GLL is -4330.809553066598 at iteration 35.\n",
      "INFO     [merf.py:307] Training GLL is -4373.655024346204 at iteration 36.\n",
      "INFO     [merf.py:307] Training GLL is -4353.8982810638445 at iteration 37.\n",
      "INFO     [merf.py:307] Training GLL is -4327.5923536475475 at iteration 38.\n",
      "INFO     [merf.py:307] Training GLL is -4349.569718680587 at iteration 39.\n",
      "INFO     [merf.py:307] Training GLL is -4336.388309709673 at iteration 40.\n",
      "INFO     [merf.py:307] Training GLL is -4345.460832093166 at iteration 41.\n",
      "INFO     [merf.py:307] Training GLL is -4353.786214764959 at iteration 42.\n",
      "INFO     [merf.py:307] Training GLL is -4353.368213384599 at iteration 43.\n",
      "INFO     [merf.py:307] Training GLL is -4348.056885938884 at iteration 44.\n",
      "INFO     [merf.py:307] Training GLL is -4362.425218407187 at iteration 45.\n",
      "INFO     [merf.py:307] Training GLL is -4357.477572020624 at iteration 46.\n",
      "INFO     [merf.py:307] Training GLL is -4300.7101602893845 at iteration 47.\n",
      "INFO     [merf.py:307] Training GLL is -4345.013915943963 at iteration 48.\n",
      "INFO     [merf.py:307] Training GLL is -4327.986814777743 at iteration 49.\n",
      "INFO     [merf.py:307] Training GLL is -4339.426929191702 at iteration 50.\n",
      "INFO     [merf.py:307] Training GLL is -4388.502108837554 at iteration 51.\n",
      "INFO     [merf.py:307] Training GLL is -4345.651922616958 at iteration 52.\n",
      "INFO     [merf.py:307] Training GLL is -4324.294452375893 at iteration 53.\n",
      "INFO     [merf.py:307] Training GLL is -4374.553731760402 at iteration 54.\n",
      "INFO     [merf.py:307] Training GLL is -4336.395204874763 at iteration 55.\n",
      "INFO     [merf.py:307] Training GLL is -4343.640696588588 at iteration 56.\n",
      "INFO     [merf.py:307] Training GLL is -4350.801841156813 at iteration 57.\n",
      "INFO     [merf.py:307] Training GLL is -4347.2648762423705 at iteration 58.\n",
      "INFO     [merf.py:307] Training GLL is -4352.7720193103905 at iteration 59.\n",
      "INFO     [merf.py:307] Training GLL is -4364.714519586795 at iteration 60.\n",
      "INFO     [merf.py:307] Training GLL is -4333.00039093369 at iteration 61.\n",
      "INFO     [merf.py:307] Training GLL is -4351.283273908571 at iteration 62.\n",
      "INFO     [merf.py:307] Training GLL is -4357.955290820856 at iteration 63.\n",
      "INFO     [merf.py:307] Training GLL is -4330.0510589129635 at iteration 64.\n",
      "INFO     [merf.py:307] Training GLL is -4329.876141144756 at iteration 65.\n",
      "INFO     [merf.py:307] Training GLL is -4338.16545924576 at iteration 66.\n",
      "INFO     [merf.py:307] Training GLL is -4330.631223011722 at iteration 67.\n",
      "INFO     [merf.py:307] Training GLL is -4345.633260698478 at iteration 68.\n",
      "INFO     [merf.py:307] Training GLL is -4355.6518795833645 at iteration 69.\n",
      "INFO     [merf.py:307] Training GLL is -4322.505151475484 at iteration 70.\n",
      "INFO     [merf.py:307] Training GLL is -4336.762883811738 at iteration 71.\n",
      "INFO     [merf.py:307] Training GLL is -4336.018711988751 at iteration 72.\n",
      "INFO     [merf.py:307] Training GLL is -4342.453446091221 at iteration 73.\n",
      "INFO     [merf.py:307] Training GLL is -4350.6075294018865 at iteration 74.\n",
      "INFO     [merf.py:307] Training GLL is -4347.342310751611 at iteration 75.\n",
      "INFO     [merf.py:307] Training GLL is -4349.706730680779 at iteration 76.\n",
      "INFO     [merf.py:307] Training GLL is -4340.7850301252165 at iteration 77.\n",
      "INFO     [merf.py:307] Training GLL is -4352.101389202646 at iteration 78.\n",
      "INFO     [merf.py:307] Training GLL is -4375.116057124723 at iteration 79.\n",
      "INFO     [merf.py:307] Training GLL is -4340.397855767065 at iteration 80.\n",
      "INFO     [merf.py:307] Training GLL is -4338.843187191838 at iteration 81.\n",
      "INFO     [merf.py:307] Training GLL is -4331.438905303826 at iteration 82.\n",
      "INFO     [merf.py:307] Training GLL is -4373.635487006354 at iteration 83.\n",
      "INFO     [merf.py:307] Training GLL is -4333.720856327463 at iteration 84.\n",
      "INFO     [merf.py:307] Training GLL is -4307.13948158623 at iteration 85.\n",
      "INFO     [merf.py:307] Training GLL is -4351.904227864855 at iteration 86.\n",
      "INFO     [merf.py:307] Training GLL is -4353.70683304366 at iteration 87.\n",
      "INFO     [merf.py:307] Training GLL is -4380.205764202945 at iteration 88.\n",
      "INFO     [merf.py:307] Training GLL is -4369.2484686621865 at iteration 89.\n",
      "INFO     [merf.py:307] Training GLL is -4325.7704020163055 at iteration 90.\n",
      "INFO     [merf.py:307] Training GLL is -4334.340070832994 at iteration 91.\n",
      "INFO     [merf.py:307] Training GLL is -4379.459117026979 at iteration 92.\n",
      "INFO     [merf.py:307] Training GLL is -4370.2126467115295 at iteration 93.\n",
      "INFO     [merf.py:307] Training GLL is -4341.043650915643 at iteration 94.\n",
      "INFO     [merf.py:307] Training GLL is -4339.276204188608 at iteration 95.\n",
      "INFO     [merf.py:307] Training GLL is -4343.48934291592 at iteration 96.\n",
      "INFO     [merf.py:307] Training GLL is -4344.410291318042 at iteration 97.\n",
      "INFO     [merf.py:307] Training GLL is -4362.0288710244995 at iteration 98.\n",
      "INFO     [merf.py:307] Training GLL is -4329.528416705964 at iteration 99.\n",
      "INFO     [merf.py:307] Training GLL is -4326.530531534801 at iteration 100.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               RMSE     Time\n",
      "GPBoost       1.261    0.145\n",
      "Linear_ME       NaN      NaN\n",
      "Boosting_Ign  1.431    0.024\n",
      "Boosting_Cat  1.290    0.045\n",
      "MERF          1.276  268.837\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = [\"RMSE\",\"Time\"],\n",
    "                       index = [\"GPBoost\", \"Linear_ME\",\"Boosting_Ign\",\"Boosting_Cat\",\"MERF\"])\n",
    "# 1. GPBoost\n",
    "gp_model = gpb.GPModel(group_data=group_train)\n",
    "start_time = time.time() # measure time\n",
    "bst = gpb.train(params=params, train_set=data_train, gp_model=gp_model, num_boost_round=best_iter)\n",
    "results.loc[\"GPBoost\",\"Time\"] = time.time() - start_time\n",
    "pred = bst.predict(data=X_test, group_data_pred=group_test)\n",
    "y_pred = pred['fixed_effect'] + pred['random_effect_mean'] # sum predictions of fixed effect and random effect\n",
    "results.loc[\"GPBoost\",\"RMSE\"] = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "\n",
    "# # 2. Linear mixed effects model ('Linear_ME')\n",
    "# gp_model = gpb.GPModel(group_data=group_train)\n",
    "# X_train_linear = np.column_stack((np.ones(ntrain),X_train))\n",
    "# X_test_linear = np.column_stack((np.ones(ntrain),X_test))\n",
    "# start_time = time.time() # measure time\n",
    "# gp_model.fit(y=y_train, X=X_train_linear) # add a column of 1's for intercept\n",
    "# results.loc[\"Linear_ME\",\"Time\"] = time.time() - start_time\n",
    "# y_pred = gp_model.predict(group_data_pred=group_test, X_pred=X_test_linear)\n",
    "# F_pred = X_test_linear.dot(gp_model.get_coef())\n",
    "# results.loc[\"Linear_ME\",\"RMSE\"] = np.sqrt(np.mean((y_test - y_pred['mu']) ** 2))\n",
    "\n",
    "# 3. Gradient tree-boosting ignoring the grouping variable ('Boosting_Ign')\n",
    "cvbst = gpb.cv(params=params, train_set=data_train,\n",
    "               num_boost_round=100, early_stopping_rounds=5,\n",
    "               nfold=4, verbose_eval=True, show_stdv=False, seed=1)\n",
    "best_iter = np.argmin(cvbst['l2-mean'])\n",
    "print(\"Best number of iterations: \" + str(best_iter))\n",
    "# Best number of iterations: 19\n",
    "start_time = time.time() # measure time\n",
    "bst = gpb.train(params=params, train_set=data_train, num_boost_round=best_iter)\n",
    "results.loc[\"Boosting_Ign\",\"Time\"] = time.time() - start_time\n",
    "y_pred = bst.predict(data=X_test)\n",
    "results.loc[\"Boosting_Ign\",\"RMSE\"] = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "\n",
    "# 4. Gradient tree-boosting including the grouping variable as a categorical variable ('Boosting_Cat')\n",
    "X_train_cat = np.column_stack((group_train,X_train))\n",
    "X_test_cat = np.column_stack((group_test,X_test))\n",
    "data_train_cat = gpb.Dataset(X_train_cat, y_train, categorical_feature=[0])\n",
    "cvbst = gpb.cv(params=params, train_set=data_train_cat,\n",
    "               num_boost_round=1000, early_stopping_rounds=5,\n",
    "               nfold=4, verbose_eval=True, show_stdv=False, seed=1)\n",
    "best_iter = np.argmin(cvbst['l2-mean'])\n",
    "print(\"Best number of iterations: \" + str(best_iter))\n",
    "# Best number of iterations: 49\n",
    "start_time = time.time() # measure time\n",
    "bst = gpb.train(params=params, train_set=data_train_cat, num_boost_round=best_iter)\n",
    "results.loc[\"Boosting_Cat\",\"Time\"] = time.time() - start_time\n",
    "y_pred = bst.predict(data=X_test_cat)\n",
    "results.loc[\"Boosting_Cat\",\"RMSE\"] = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "\n",
    "# 5. Mixed-effects random forest ('MERF')\n",
    "from merf import MERF\n",
    "# rf_params={'max_depth': 6, 'n_estimators': 300}\n",
    "merf_model = MERF(max_iterations=100)\n",
    "print(\"Warning: the following takes a lot of time\")\n",
    "start_time = time.time() # measure time\n",
    "merf_model.fit(pd.DataFrame(X_train), np.ones(shape=(ntrain,1)), pd.Series(group_train), y_train)\n",
    "results.loc[\"MERF\",\"Time\"] = time.time() - start_time\n",
    "y_pred = merf_model.predict(pd.DataFrame(X_test), np.ones(shape=(ntrain,1)), pd.Series(group_test))\n",
    "results.loc[\"MERF\",\"RMSE\"] = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "\n",
    "print(results.apply(pd.to_numeric).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
