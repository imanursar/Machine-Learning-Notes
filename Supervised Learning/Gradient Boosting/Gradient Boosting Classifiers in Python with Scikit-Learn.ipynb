{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifiers in Python with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are usually used when doing gradient boosting. Gradient boosting models are becoming popular because of their effectiveness at classifying complex datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Gradient Boosting Came to Be "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind \"gradient boosting\" is to take a weak hypothesis or weak learning algorithm and make a series of tweaks to it that will improve the strength of the hypothesis/learner. This type of Hypothesis Boosting is based on the idea of Probability Approximately Correct Learning. \n",
    "\n",
    "this PAC learning method investigates machine learning problems to interpret how complex they are, and a similiar method is applied to Hypothesis Boosting.\n",
    "\n",
    "in hypothesis boosting, you look at all observations that machine learning aglorithm is trained on, and you leave only the observations that the machine learning method successfully classified behind, stripping out the other observations. A new weak learner is created and tested on the set of data that was poorly classified, and then just the examples that were successfully classified and kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for AdaBoost, many weak learners are created by initializing many decision tree algorithms that only have a single split, such as the \"stump\".\n",
    "\n",
    "the instances/observations in the training set are weighted by the algorithm, and more weight is assigned to instances which are difficult to classify. more weak learners are added into the system sequentially, and they assigned to the most difficult training instances.\n",
    "\n",
    "in Adaboost, the predictions are made through majority vote, with the instances being classified according to which class receives the most votes from the weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting classifiers are the Ada Boosting method combined with weighted minimization, after which the classifiers and weighted inputs are recalculated. The objective of Gradient Boosting classifiers is to minimize the loss, or the difference between the actual class value of the training example and the predicted class value. It isn't required to understand the process for reducing the classifier's loss, but it operates similarly to gradient descent. \n",
    "\n",
    "In the case of Gradient Boosting Machines, every time a new weak learner is added to the model, the weights of the previous learners are frozen or cemented in place, left unchanged as the new layers are introduced. This is distinct from the approaches used in Ada Boosting where the values are adjusted when new learners are added. \n",
    "\n",
    "The power of gradient boosting machines comes from the fact that they can be used on more than binary classification problems, they can be used on multiclass classification problems and even regression problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Behind Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting Classifier depends on a loss function. A custom loss function can be used, and many standardized loss functions are supported by gradient boosting classifiers, but the loss function has to be differentiable. \n",
    "\n",
    "Classification algorithms frequently use logarithmic loss, while regression algorithms can use squared errors. Gradient boosting systems don't have to derive a new loss function every time the boosting algorithm is added, rather any differentiable loss function can be applied to the system. \n",
    "\n",
    "Gradient boosting systems have two other necessary parts: a weak learner and an additive component. Gradient boosting systems use decision trees as their weak learners. Regression trees are used for the weak learners, and thes regression trees output real values. because the outputs are real values, as new learners are added into the model the output of the regression trees can be added together to correct for errors in the predictions.\n",
    "\n",
    "the additive component of the gradient boosting model comes from the fact that trees are added to the model over time, and when this occurs the existing trees arent manipulated, their values remain fixed.\n",
    "\n",
    "a procedure similiar to gradient descent is used to minimize the error between given parameters. this is done by taking the calculated loss and preforming gradient descent to reduce that loss. Afterward, the parameters of the tree are modified ot reduce the residual loss.\n",
    "\n",
    "the new tree's output is then append to the output of the previous trees used in the model. this process is repeated until a previously specified number of trees is reached or the loss is reduced below a centain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a trade-off between the learning rate and the number of trees needed, so you'll have to experiment to find the best values for each of the parameters, but small values less than 0.1 or values between 0.1 and 0.3 often work well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read and edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data[\"Survived\"]\n",
    "train_data.drop(labels=\"Survived\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = train_data.append(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\"Name\", \"Age\", \"SibSp\", \"Ticket\", \"Cabin\", \"Parch\", \"Embarked\"]\n",
    "full_data.drop(labels=drop_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.get_dummies(full_data, columns=[\"Sex\"])\n",
    "full_data.fillna(value=0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = full_data.values[0:891]\n",
    "X_test = full_data.values[891:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 12 \n",
    "test_size = 0.30 \n",
    " \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  test_size=test_size, random_state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.05\n",
      "Accuracy score (training): 0.801\n",
      "Accuracy score (validation): 0.731\n",
      "\n",
      "Learning rate:  0.075\n",
      "Accuracy score (training): 0.814\n",
      "Accuracy score (validation): 0.731\n",
      "\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.812\n",
      "Accuracy score (validation): 0.724\n",
      "\n",
      "Learning rate:  0.25\n",
      "Accuracy score (training): 0.835\n",
      "Accuracy score (validation): 0.750\n",
      "\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.864\n",
      "Accuracy score (validation): 0.772\n",
      "\n",
      "Learning rate:  0.75\n",
      "Accuracy score (training): 0.875\n",
      "Accuracy score (validation): 0.754\n",
      "\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.875\n",
      "Accuracy score (validation): 0.739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "for learning_rate in lr_list:\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=20, \n",
    "                                        learning_rate=learning_rate, \n",
    "                                        max_features=2, max_depth=2, \n",
    "                                        random_state=0)\n",
    "    \n",
    "    gb_clf.fit(X_train, y_train)\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\\n\".format(gb_clf.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[142  19]\n",
      " [ 42  65]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       161\n",
      "           1       0.77      0.61      0.68       107\n",
      "\n",
      "    accuracy                           0.77       268\n",
      "   macro avg       0.77      0.74      0.75       268\n",
      "weighted avg       0.77      0.77      0.77       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb_clf2 = GradientBoostingClassifier(n_estimators=20, learning_rate=0.5, \n",
    "                                     max_features=2, max_depth=2, random_state=0)\n",
    "gb_clf2.fit(X_train, y_train)\n",
    "predictions = gb_clf2.predict(X_val)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
