{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes.\n",
    "This is called large margin classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are sensitive to the feature scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft and hard MarginClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we strictly impose that all instances be off the street and on the right side, this is\n",
    "called hard margin classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main issues with hard margin classification. First, it only works if the data is linearly separable, and second it is quite sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to find a good balance between keeping the street as large as possible and limiting the\n",
    "margin violations (i.e., instances that end up in the middle of the street or even on the\n",
    "wrong side). This is called so margin classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "control this balance using the C hyperparameter: a smaller C value leads to a wider street but more margin violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left, using a high C value the classifier makes fewer margin violations but ends up with a smaller margin. On the right, using a low C value the margin is much larger, but many instances end up on the street."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in fact even on this training set it makes fewer prediction errors, since most of the margin violations are actually on the correct side of the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your SVM model is overfitting, you can try regularizing it by reducing C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1), but it is much slower, especially with large training sets, so it is not recommended. \n",
    "\n",
    "Another option is to use the SGDClassifier class, with SGDClassifier(loss=\"hinge\",alpha=1/(m*C)). \n",
    "This applies regular Stochastic Gradient Descent (see Chapter 4) to train a linear SVM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not converge as fast as the LinearSVC class, but it can be useful to handle huge datasets that do not fit in memory (out-of-core training), or to handle online classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this idea using Scikit-Learn, you can create a Pipeline containing a PolynomialFeatures transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at a low polynomial degree it cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when using SVMs you can apply an almost miraculous mathematical\n",
    "technique called the kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes it possible to get the same result as if you added many polynomial features, even with very highdegree polynomials, without actually having to add them. So there is no combinatorial explosion of the number of features since you don’t actually add any features. This trick is implemented by the SVC class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if your model is overfitting, you might want to reduce the polynomial degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if your model is overfitting, you might want to reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing it. The hyperparameter coef0 controls how much the model is influenced by highdegree polynomials versus low-degree polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Similarity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique to tackle nonlinear problems is to add features computed using a\n",
    "similarity function that measures how much each instance resembles a particular\n",
    "landmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest approach is to create a landmark at the location of each and every instance in the dataset. This creates many dimensions and thus increases the chances that the transformed training set will be\n",
    "linearly separable. The downside is that a training set with m instances and n features\n",
    "gets transformed into a training set with m instances and m features (assuming you\n",
    "drop the original features). If your training set is very large, you end up with an\n",
    "equally large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the kernel trick does its SVM magic: it makes it possible to obtain a similar\n",
    "result as if you had added many similarity features, without actually having to add\n",
    "them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing gamma makes the bell-shape curve narrower, and as a\n",
    "result each instance’s range of influence is smaller: the decision boundary ends up\n",
    "being more irregular, wiggling around individual instances. Conversely, a small gamma\n",
    "value makes the bell-shaped curve wider, so instances have a larger range of influ‐\n",
    "ence, and the decision boundary ends up smoother. So γ acts like a regularization\n",
    "hyperparameter: if your model is overfitting, you should reduce it, and if it is under‐\n",
    "fitting, you should increase it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb, you should always try the linear kernel first (remember that LinearSVC is much faster than SVC(kernel=\"linear\")), especially if the training set is very large or if it\n",
    "has plenty of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also experiment with a few other kernels using cross-validation and grid search, especially if there are kernels specialized for your training set’s data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not support the kernel trick, but it scales almost linearly with the number of training instances and the number of features: its training time complexity is roughly O(m × n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm takes longer if you require a very high precision. This is controlled by\n",
    "the tolerance hyperparameter ϵ (called tol in Scikit-Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC class is based on the libsvm library, which implements an algorithm that sup‐\n",
    "ports the kernel trick.2 The training time complexity is usually between O(m2 × n)\n",
    "and O(m3 × n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Function and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear SVM classifier model predicts the class of a new instance x by simply com‐\n",
    "puting the decision function wT · x + b = w1 x1 + ⋯ + wn xn + b: if the result is posi‐\n",
    "tive, the predicted class ŷ is the positive class (1), or else it is the negative class (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is the set of points where the decision\n",
    "function is equal to 0: it is the intersection of two planes, which is a straight line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashed lines represent the points where the decision function is equal to 1 or –1:\n",
    "they are parallel and at equal distance to the decision boundary, forming a margin\n",
    "around it. Training a linear SVM classifier means finding the value of w and b that\n",
    "make this margin as wide as possible while avoiding margin violations (hard margin)\n",
    "or limiting them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, when there are n features, the decision function is an n-dimensional hyperplane, and the decision boundary is an (n – 1)-dimensional hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hard margin objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the slope of the decision function: it is equal to the norm of the weight vec‐\n",
    "tor, ∥ w ∥. The smaller the weight vector w, the larger the margin. So we want to minimize ∥ w ∥ to get a large margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we also want to avoid any margin violation (hard margin), then we need the decision function to be greater than 1 for all positive training instances, and lower than –1 for negative training\n",
    "instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we define t(i) = –1 for negative instances (if y(i) = 0) and t(i) = 1 for positive\n",
    "instances (if y(i) = 1), then we can express this constraint as t(i)(wT · x(i) + b) ≥ 1 for all\n",
    "instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the hard margin linear SVM classifier objective as the constrained optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### soft margin objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the soft margin objective, we need to introduce a slack variable ζ(i) ≥ 0 for each\n",
    "instance:4 ζ(i) measures how much the ith instance is allowed to violate the margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two conflicting objectives: making the slack variables as small as possible to\n",
    "reduce the margin violations, and making 1 2 wT · w as small as possible to increase the\n",
    "margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mensedikitkan data yang boleh melewati batas, dengan memperkecil w (melebarkan batas yang ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the C hyperparameter comes in: it allows us to define the trade\n",
    "off between these two objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
