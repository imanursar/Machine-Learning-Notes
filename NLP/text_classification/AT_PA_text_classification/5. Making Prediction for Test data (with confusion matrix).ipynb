{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "# NLP library\n",
    "import nltk\n",
    "import re\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, roc_auc_score, auc, classification_report\n",
    "\n",
    "# function\n",
    "import sys\n",
    "sys.path.append('function/')\n",
    "from ursar import nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "def preprocess_text(sentence):\n",
    "    id_stop = set(nltk.corpus.stopwords.words('indonesian'))\n",
    "    factory_Stemmer = StemmerFactory()\n",
    "    stemmer = factory_Stemmer.create_stemmer()\n",
    "    sentence = re.sub(r'\\W', ' ', str(sentence))\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    sentence = re.sub(r'\\^[a-zA-Z]\\s+', ' ', sentence)\n",
    "    sentence = sentence.replace(\"\\n\",\" \")\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    sentence = re.sub(r\"\\s+$\", \"\", sentence)\n",
    "    sentence = re.sub(r\"^\\s+\", \"\", sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.split()\n",
    "    sentence = [word for word in sentence if word not in id_stop]\n",
    "    sentence = [word for word in sentence if len(word) > 3]\n",
    "    sentence = ' '.join(sentence)\n",
    "    sentence = stemmer.stem(sentence)\n",
    "    return sentence\n",
    "\n",
    "def tokenize_matrix(corpus,mode):\n",
    "    #corpus, 5000, 'post', 120\n",
    "    # create the tokenizer\n",
    "    # load label train dataset file here\n",
    "    with open('model/tokenizer_ann', 'rb') as picklefile:\n",
    "        tokenizer = pickle.load(picklefile)\n",
    "\n",
    "    # encode training data set\n",
    "    sen = tokenizer.texts_to_matrix(corpus, mode=mode)\n",
    "    return(sen)\n",
    "\n",
    "def tokenize_embedding(corpus,padding_type,max_length):\n",
    "    #corpus, 5000, 'post', 120\n",
    "    # create the tokenizer\n",
    "    # load label train dataset file here\n",
    "    with open('model/tokenizer_embed', 'rb') as picklefile:\n",
    "        tokenizer = pickle.load(picklefile)\n",
    "\n",
    "    # encode training data set\n",
    "    sen = tokenizer.texts_to_sequences(corpus)\n",
    "    sen = pad_sequences(sen, padding=padding_type, maxlen=max_length)\n",
    "    return(sen)\n",
    "\n",
    "def predict_embedding(reviews, model,input):\n",
    "    # apply preprocess_text function to out training dataset\n",
    "    # encode\n",
    "    if (input == \"embedding\"):\n",
    "        encoded = tokenize_embedding(reviews,'post', 120)\n",
    "    if (input == \"matrixs\"):\n",
    "        encoded = tokenize_matrix(reviews,\"freq\")\n",
    "    # prediction\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "\n",
    "    if (yhat[0,0]>=0.5):\n",
    "        res = \"positive review\"\n",
    "    else:\n",
    "        res = \"negative review\"\n",
    "    return (res,yhat[0,0])\n",
    "\n",
    "def print_score (y_test,y_pred,y_probs):\n",
    "    print(\"comfusion matrix = \")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    (tn,fp,fn,tp) = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    print(\"\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('accuracy_score = ', accuracy)\n",
    "    bas = balanced_accuracy_score(y_test, y_pred)\n",
    "    print('balanced_accuracy_score = ', bas)\n",
    "    #balanced accuracy is equal to the arithmetic mean of sensitivity (true positive rate) and specificity (true negative rate),\n",
    "    #or the area under the ROC curve with binary predictions rather than scores.\n",
    "\n",
    "    #In multilabel classification,\n",
    "    #this function computes subset accuracy: the set of labels predicted for\n",
    "    #a sample must exactly match the corresponding set of labels in y_true\n",
    "\n",
    "    print(\"\")\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    aps = average_precision_score(y_test, y_pred)\n",
    "    print (\"precision score = \", precision)\n",
    "    print (\"average precision score = \", aps)\n",
    "    print (\"recall score = \", recall)\n",
    "\n",
    "    #precision An interesting one to look at is the accuracy of the positive pre‚Äê dictions; this is called the precision of the classifier\n",
    "    # recall, also called sensitivity or true positive rate (TPR): this is the ratio of positive instances that are correctly detected by the classifier\n",
    "    #precision = TP/TP + FP\n",
    "    #recall = TP/TP + FN\n",
    "\n",
    "    print(\"\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print (\"F1 score = \", f1)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "    aucs = auc(recall, precision)\n",
    "    print (\"AUC of Precision-Recall Curve on Testing = \", aucs)\n",
    "    aucroc = roc_auc_score(y_test,y_probs)\n",
    "    print (\"AUC of ROC = \", aucroc)\n",
    "    gini = aucs*2 - 1\n",
    "    print(\"Gini = \", gini)\n",
    "\n",
    "    print(\"\")\n",
    "    cr = classification_report(y_test,y_pred)\n",
    "    print(\"classification_report\")\n",
    "    print(cr)\n",
    "\n",
    "    #The F1 score is the harmonic mean of precision and recall (Equation 3-3).\n",
    "    #Whereas the regular mean treats all values equally,\n",
    "    #the harmonic mean gives much more weight to low values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Test dataset\n",
      "\n",
      "data testing shape\n",
      "(185, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Load Test dataset\")\n",
    "test = pd.read_csv('DATA/test_data_restaurant.tsv', sep='\\t',header=None).sample(frac=1).reset_index(drop=True)\n",
    "test.columns = ['sentence', 'label']\n",
    "print(\"\\ndata testing shape\")\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "is the test dataset contain the null values?\n",
      "sentence    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nis the test dataset contain the null values?\")\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "change label for test dataset\n"
     ]
    }
   ],
   "source": [
    "test[\"label\"] = list(map(lambda x: 1 if x==\"positive\" else 0, test[\"label\"]))\n",
    "print(\"\\nchange label for test dataset\")\n",
    "test[\"label\"].unique()\n",
    "y_test = test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "apply preprocess_text function to out testing dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"\\napply preprocess_text function to out testing dataset\")\n",
    "reviews_test = []\n",
    "sentences = list(test[\"sentence\"])\n",
    "for sen in sentences:\n",
    "    reviews_test.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "make token for test dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nmake token for test dataset\")\n",
    "token_test_ann = tokenize_matrix(reviews_test,\"freq\")\n",
    "token_test = tokenize_embedding(reviews_test,'post', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "load matrix_ANN model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nload matrix_ANN model\")\n",
    "model = load_model('model/model_matrix_ANN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy:  0.7892\n",
      "Testing loss:  0.4885\n",
      "\n",
      "comfusion matrix = \n",
      "[[ 32  33]\n",
      " [  6 114]]\n",
      "\n",
      "accuracy_score =  0.7891891891891892\n",
      "balanced_accuracy_score =  0.7211538461538461\n",
      "\n",
      "precision score =  0.7755102040816326\n",
      "average precision score =  0.7691671263099834\n",
      "recall score =  0.95\n",
      "\n",
      "F1 score =  0.8539325842696629\n",
      "AUC of Precision-Recall Curve on Testing =  0.8606574410209081\n",
      "AUC of ROC =  0.8242307692307692\n",
      "Gini =  0.7213148820418163\n",
      "\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.49      0.62        65\n",
      "           1       0.78      0.95      0.85       120\n",
      "\n",
      "    accuracy                           0.79       185\n",
      "   macro avg       0.81      0.72      0.74       185\n",
      "weighted avg       0.80      0.79      0.77       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(token_test_ann,y_test , verbose=False)\n",
    "print(\"\\nTesting Accuracy:  {:.4f}\".format(accuracy))\n",
    "print(\"Testing loss:  {:.4f}\\n\".format(loss))\n",
    "print_score(y_test,np.round(model.predict(token_test_ann)),model.predict(token_test_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "load CNN model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nload CNN model\")\n",
    "model = load_model('model/model_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy:  0.8270\n",
      "Testing loss:  0.5228\n",
      "\n",
      "comfusion matrix = \n",
      "[[ 43  22]\n",
      " [ 10 110]]\n",
      "\n",
      "accuracy_score =  0.827027027027027\n",
      "balanced_accuracy_score =  0.7891025641025641\n",
      "\n",
      "precision score =  0.8333333333333334\n",
      "average precision score =  0.8179429429429429\n",
      "recall score =  0.9166666666666666\n",
      "\n",
      "F1 score =  0.8730158730158729\n",
      "AUC of Precision-Recall Curve on Testing =  0.8840873698179894\n",
      "AUC of ROC =  0.8451282051282051\n",
      "Gini =  0.7681747396359788\n",
      "\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.66      0.73        65\n",
      "           1       0.83      0.92      0.87       120\n",
      "\n",
      "    accuracy                           0.83       185\n",
      "   macro avg       0.82      0.79      0.80       185\n",
      "weighted avg       0.83      0.83      0.82       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(token_test,y_test, verbose=False)\n",
    "print(\"\\nTesting Accuracy:  {:.4f}\".format(accuracy))\n",
    "print(\"Testing loss:  {:.4f}\\n\".format(loss))\n",
    "print_score(y_test,np.round(model.predict(token_test)),model.predict(token_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "load CNN LSTM model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nload CNN LSTM model\")\n",
    "model = load_model('model/model_CNN_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy:  0.7946\n",
      "Testing loss:  0.5262\n",
      "\n",
      "comfusion matrix = \n",
      "[[ 47  18]\n",
      " [ 20 100]]\n",
      "\n",
      "accuracy_score =  0.7945945945945946\n",
      "balanced_accuracy_score =  0.7782051282051282\n",
      "\n",
      "precision score =  0.847457627118644\n",
      "average precision score =  0.8143227973736449\n",
      "recall score =  0.8333333333333334\n",
      "\n",
      "F1 score =  0.8403361344537815\n",
      "AUC of Precision-Recall Curve on Testing =  0.9035301805555064\n",
      "AUC of ROC =  0.8628205128205128\n",
      "Gini =  0.8070603611110128\n",
      "\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.72      0.71        65\n",
      "           1       0.85      0.83      0.84       120\n",
      "\n",
      "    accuracy                           0.79       185\n",
      "   macro avg       0.77      0.78      0.78       185\n",
      "weighted avg       0.80      0.79      0.80       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(token_test,y_test, verbose=False)\n",
    "print(\"\\nTesting Accuracy:  {:.4f}\".format(accuracy))\n",
    "print(\"Testing loss:  {:.4f}\\n\".format(loss))\n",
    "print_score(y_test,np.round(model.predict(token_test)),model.predict(token_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "load CNN wiki model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nload CNN wiki model\")\n",
    "model = load_model('model/model_CNN_wiki.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy:  0.7730\n",
      "Testing loss:  0.5976\n",
      "\n",
      "comfusion matrix = \n",
      "[[ 31  34]\n",
      " [  8 112]]\n",
      "\n",
      "accuracy_score =  0.772972972972973\n",
      "balanced_accuracy_score =  0.7051282051282052\n",
      "\n",
      "precision score =  0.7671232876712328\n",
      "average precision score =  0.7592249784030606\n",
      "recall score =  0.9333333333333333\n",
      "\n",
      "F1 score =  0.8421052631578947\n",
      "AUC of Precision-Recall Curve on Testing =  0.8765774603184949\n",
      "AUC of ROC =  0.816923076923077\n",
      "Gini =  0.7531549206369899\n",
      "\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.48      0.60        65\n",
      "           1       0.77      0.93      0.84       120\n",
      "\n",
      "    accuracy                           0.77       185\n",
      "   macro avg       0.78      0.71      0.72       185\n",
      "weighted avg       0.78      0.77      0.76       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(token_test,y_test, verbose=False)\n",
    "print(\"\\nTesting Accuracy:  {:.4f}\".format(accuracy))\n",
    "print(\"Testing loss:  {:.4f}\\n\".format(loss))\n",
    "print_score(y_test,np.round(model.predict(token_test)),model.predict(token_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "load CNN LSTM wiki model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nload CNN LSTM wiki model\")\n",
    "model = load_model('model/model_CNN_LSTM_wiki.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy:  0.7676\n",
      "Testing loss:  0.6052\n",
      "\n",
      "comfusion matrix = \n",
      "[[ 36  29]\n",
      " [ 14 106]]\n",
      "\n",
      "accuracy_score =  0.7675675675675676\n",
      "balanced_accuracy_score =  0.7185897435897436\n",
      "\n",
      "precision score =  0.7851851851851852\n",
      "average precision score =  0.7692559225892559\n",
      "recall score =  0.8833333333333333\n",
      "\n",
      "F1 score =  0.8313725490196078\n",
      "AUC of Precision-Recall Curve on Testing =  0.8986731472238412\n",
      "AUC of ROC =  0.8398717948717946\n",
      "Gini =  0.7973462944476823\n",
      "\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.55      0.63        65\n",
      "           1       0.79      0.88      0.83       120\n",
      "\n",
      "    accuracy                           0.77       185\n",
      "   macro avg       0.75      0.72      0.73       185\n",
      "weighted avg       0.76      0.77      0.76       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(token_test,y_test, verbose=False)\n",
    "print(\"\\nTesting Accuracy:  {:.4f}\".format(accuracy))\n",
    "print(\"Testing loss:  {:.4f}\\n\".format(loss))\n",
    "print_score(y_test,np.round(model.predict(token_test)),model.predict(token_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
