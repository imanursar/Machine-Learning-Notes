{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load all necessary libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense,LSTM,Bidirectional\n",
    "from tensorflow.keras.layers import Flatten,Input\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D,Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding,concatenate\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import bert\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# function\n",
    "import sys\n",
    "sys.path.append('function/')\n",
    "from ursar import nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = pd.read_csv('DATA/test_data_restaurant_rest.tsv', sep='\\t',header=None)\n",
    "test.columns = ['sentence', 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing before translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    # Remove all the special characters\n",
    "    # remove all the non-word characters (letters and numbers) from a string and keep the remaining characters\n",
    "    sentence = re.sub(r'\\W', ' ', str(sentence))\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    \n",
    "    # Single character removal\n",
    "    # Sometimes removing punctuation marks, such as an apostrophe, results in a single character which has no meaning\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    sentence = re.sub(r'\\^[a-zA-Z]\\s+', ' ', sentence)\n",
    "    \n",
    "    # Remove enter type to space\n",
    "    sentence = sentence.replace(\"\\n\",\" \")\n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    #removes spaces from the start and end \n",
    "    sentence = re.sub(r\"\\s+$\", \"\", sentence)\n",
    "    sentence = re.sub(r\"^\\s+\", \"\", sentence)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    sentence = sentence.lower()\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translatedList = []\n",
    "for index, row in test.iterrows():\n",
    "    # REINITIALIZE THE API\n",
    "    translator = Translator()\n",
    "    newrow = copy.deepcopy(row)\n",
    "    try:\n",
    "        # translate the 'text' column\n",
    "        translated = translator.translate(preprocess_text(row['sentence']), dest='en')\n",
    "        newrow['translated'] = translated.text\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        continue\n",
    "    translatedList.append(newrow)\n",
    "    print(newrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwer = pd.DataFrame(translatedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwer[\"translated\"].to_csv('DATA/test_raw_translate.txt',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(translatedList).to_csv('DATA/train_raw_translate.txt',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data training shape\n",
      "(1780, 1)\n",
      "data testing shape\n",
      "(185, 1)\n"
     ]
    }
   ],
   "source": [
    "train  = pd.read_csv('DATA/train_raw_translate.txt', header=None)\n",
    "test = pd.read_csv('DATA/test_raw_translate.txt', header=None)\n",
    "train.columns = ['sentence']\n",
    "test.columns = ['sentence']\n",
    "print(\"data training shape\")\n",
    "print(train.shape)\n",
    "print(\"data testing shape\")\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load label train dataset file here\n",
    "with open('DATA/label_train', 'rb') as picklefile:\n",
    "    y_train = pickle.load(picklefile)\n",
    "\n",
    "# load label test dataset file here\n",
    "with open('DATA/label_test', 'rb') as picklefile:\n",
    "    y_test = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Converting to Lowercase\n",
    "    sentence = sen.lower()\n",
    "    # Lemmatization\n",
    "    # reduce the word into dictionary root form\n",
    "    sentence = sentence.split()\n",
    "    sentence = [stemmer.lemmatize(word) for word in sentence]\n",
    "    sentence = ' '.join(sentence)\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "sentences = list(train['sentence'])\n",
    "for sen in sentences:\n",
    "    X_train.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "sentences = list(test['sentence'])\n",
    "for sen in sentences:\n",
    "    X_test.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a BERT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use BERT text embeddings as input to train text classification model, we need to tokenize our text reviews. Tokenization refers to dividing a sentence into individual words. To tokenize our text, we will be using the BERT tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above we first \n",
    "\n",
    "1. create an object of the FullTokenizer class from the bert.bert_tokenization module. \n",
    "2. create a BERT embedding layer by importing the BERT model from hub.KerasLayer. \n",
    "    The trainable parameter is set to False, which means that we will not be training the BERT embedding. \n",
    "3. create a BERT vocabulary file in the form a numpy array. \n",
    "4. set the text to lowercase \n",
    "5. pass our vocabulary_file and to_lower_case variables to the BertTokenizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2123, 2102, 2022, 2061, 8689, 2389]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize a random sentence\n",
    "tokenizer.tokenize(\"don't be so judgmental\")\n",
    "# get the ids of the tokens using the convert_tokens_to_ids() of the tokenizer object\n",
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"dont be so judgmental\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(text_reviews):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to actually tokenize all the reviews in the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [tokenize_reviews(review) for review in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = [tokenize_reviews(review) for review in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data For Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, the input sentences should be of equal length. \n",
    "To create sentences of equal length, one way is to pad the shorter sentences by 0s. \n",
    "However, this can result in a sparse matrix contain large number of 0s. \n",
    "The other way is to pad sentences within each batch. Since we will be training the model in batches, \n",
    "we can pad the sentences within the training batch locally depending upon the length of the longest sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list contains tokenized review, the label of the review and \n",
    "#length of the review:\n",
    "reviews_with_len = [[review, y_train[i], len(review)]\n",
    "                 for i, review in enumerate(tokenized_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the review\n",
    "import random\n",
    "random.shuffle(reviews_with_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the data by the length of the reviews\n",
    "#sort base on third columns\n",
    "reviews_with_len.sort(key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the length attribute from all the reviews\n",
    "sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert dataset for train TensorFlow 2.0 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-af868cbd9bc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train TensorFlow 2.0 models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprocessed_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msorted_reviews_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# train TensorFlow 2.0 models\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pad  dataset for each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch size we are going to use is 32 which means that after processing 32 reviews, the weights of the neural network will be updated and pad the reviews locally with respect to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 21), dtype=int32, numpy=\n",
       " array([[ 3191,  1996,  2338,  5293,  1996,  3185,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 3078,  5436,  3078,  3257,  3532,  7613,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2054,  5896,  2054,  2466,  2054,  6752,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2062, 23873,  3993,  2062, 11259,  2172,  2172,  2062, 14888,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2876,  9278,  2023,  2028,  2130,  2006,  7922, 12635,  2305,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  3185,  2003,  6659,  2021,  2009,  2038,  2070,  2204,\n",
       "          3896,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 3246,  2023,  2177,  1997,  2143, 11153,  2196,  2128, 15908,\n",
       "          2015,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 8235,  1998,  3048,  4616,  2011,  3419,  2457, 27727,  1998,\n",
       "          2848, 16133,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 7918, 14674,  7662,  2003,  6581,  2003,  2023,  2143,  2002,\n",
       "          3084, 17160,  2450,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [11861,  1996, 21442,  6895,  3238,  2515,  2210, 22759,  6198,\n",
       "          1998,  3185,  2087, 12487,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  2307,  3185,  2205,  2919,  2009,  2003,  2025,\n",
       "          2800,  2006,  2188,  2678,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2017,  2488,  5454,  2703,  2310, 25032,  8913,  8159,  2130,\n",
       "          2065,  2017,  2031,  3427,  2009,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2053,  7615,  5236,  3185,  3772,  2779,  2030,  4788,  9000,\n",
       "          2053,  3168,  2012,  2035, 13558,  2009,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2123,  2113,  2339,  2066,  2023,  3185,  2061,  2092,  2021,\n",
       "          2196,  2131,  5458,  1997,  3666,  2009,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2146, 11771,  1038,  8523,  8458,  6633,  3560,  2196,  2031,\n",
       "          2042,  2061,  5580,  2000,  2156,  4566,  6495,  4897,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2074,  2293,  1996,  6970, 13068,  2090,  2048,  2307,  3494,\n",
       "          1997,  2754,  3898,  2310,  3593,  2102,  6287,  5974,     0,\n",
       "             0,     0,     0],\n",
       "        [ 7615,  2023,  3185,  2003,  5263,  2003,  6659,  2200, 17727,\n",
       "          3217,  3676,  3468,  2919,  7613,  3257,  2025,  2298,     0,\n",
       "             0,     0,     0],\n",
       "        [ 7244,  3185,  2009,  2003,  2440,  1997,  6699,  1998,  6919,\n",
       "          3772,  2071,  2031,  2938,  2083,  2009,  2117,  2051,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  1996, 15764,  3185,  2544,  1997,  8429, 24905,\n",
       "         17988,  7659,  2498,  2021,  2045,  2024,  2053, 13842,  5312,\n",
       "             0,     0,     0],\n",
       "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
       "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
       "             0,     0,     0],\n",
       "        [ 2033,  6491, 11124,  6774,  2143,  2008,  5121,  7906,  2115,\n",
       "          3086,  3841, 13196,  2003, 17160,  1998, 26103,  2000,  3422,\n",
       "             0,     0,     0],\n",
       "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
       "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  2204,  2143,  2023,  2003,  2200,  6057,  2664,\n",
       "          2044,  2023,  2143,  2045,  2020,  2053,  2204,  8471,  3152,\n",
       "             0,     0,     0],\n",
       "        [ 5790,  1997,  2515,  2025,  4088,  2000,  4671,  2129, 10634,\n",
       "          2139, 24128,  1998, 21660,  2135,  2919,  2023,  3185,  2003,\n",
       "             0,     0,     0],\n",
       "        [ 5587,  2023,  2210, 17070,  2000,  2115,  2862,  1997,  6209,\n",
       "         24945,  2009, 26354, 28394,  2102,  6057,  1998,  2203, 27242,\n",
       "             0,     0,     0],\n",
       "        [ 7078, 10392,  3649,  2360,  2876,  2079,  2023,  2104,  9250,\n",
       "          3185,  1996,  3425,  2009, 17210,  3422,  2009,  2085, 10392,\n",
       "             0,     0,     0],\n",
       "        [ 6283,  2009,  2007,  2035,  2026,  2108,  5409,  3185,  2412,\n",
       "         10597, 21985,  2393,  2033,  2009,  2001,  2008,  2919,  3404,\n",
       "          2033,     0,     0],\n",
       "        [ 7244,  2092,  2856, 10828,  1997, 10904,  2402,  2472,  3135,\n",
       "          2293,  2466,  2007, 10958,  8428, 10102,  1999,  1996,  4281,\n",
       "          4276,  3773,     0],\n",
       "        [ 2005,  5760,  7788,  4393,  8808,  2498,  2064, 12826,  2000,\n",
       "          1996, 11056,  3152,  3811, 16755,  2169,  1998,  2296,  2028,\n",
       "          1997,  2068,     0],\n",
       "        [ 2023,  3185,  2097,  2467,  2022,  5934,  1998,  3185,  4438,\n",
       "          2004,  2146,  2004,  2045,  2024,  2145,  2111,  2040,  6170,\n",
       "          3153,  1998,  2552],\n",
       "        [ 2023,  2003,  6659,  3185,  2123,  5949,  2115,  2769,  2006,\n",
       "          2009,  2123,  2130,  3422,  2009,  2005,  2489,  2008,  2035,\n",
       "          2031,  2000,  2360],\n",
       "        [ 2028,  1997,  1996,  4569, 15580,  2102,  5691,  2081,  1999,\n",
       "          3522,  2086,  2204, 23191,  5436,  1998, 11813,  6370,  2191,\n",
       "          2023,  2028,  4438]])>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 1, 1, 0, 1])>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first batch and see how padding has been applied to it\n",
    "next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the last five reviews, you can see that the total number of words in the largest sentence were 21.\n",
    "\n",
    "in the first five reviews the 0s are added at the end of the sentences so that their total length is also 21. The padding for the next batch will be different depending upon the size of the largest sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
