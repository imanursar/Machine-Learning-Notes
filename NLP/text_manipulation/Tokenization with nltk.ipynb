{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization may be defined as the process of splitting the text into smaller parts called tokens, and is considered a crucial step in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "from nltk.tokenize import LineTokenizer\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "from nltk.tokenize.util import spans_to_relative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ursar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input data text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "text_1=\" She secured 90.56 % in class X \\n. She is a meritorious student\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', 'readers', '.', 'I', 'hope', 'you', 'find', 'it', 'interesting', '.', 'Please', 'do', 'reply', '.']\n"
     ]
    }
   ],
   "source": [
    "texts=nltk.word_tokenize(text)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of text is 14 words\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of text is\",len(word_tokenize(text)),\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization using TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'readers.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'it',\n",
       " 'interesting.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'reply',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', 'readers', '.', 'I', 'hope', 'you', 'find', 'it', 'interesting', '.', 'Please', 'do', 'reply', '.']\n"
     ]
    }
   ],
   "source": [
    "texts=nltk.word_tokenize(text)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another word tokenizer is PunktWordTokenizer. It works by splitting punctuation;\n",
    "each word is kept instead of creating an entirely new token. Another word tokenizer\n",
    "is WordPunctTokenizer. It provides splitting by making punctuation an entirely\n",
    "new token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'readers',\n",
       " '.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'it',\n",
       " 'interesting',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'reply',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=WordPunctTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization of words can be performed by constructing regular expressions in\n",
    "these two ways:\n",
    "• By matching with words\n",
    "• By matching spaces or gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'secured',\n",
       " '90',\n",
       " '56',\n",
       " 'in',\n",
       " 'class',\n",
       " 'X',\n",
       " 'She',\n",
       " 'is',\n",
       " 'a',\n",
       " 'meritorious',\n",
       " 'student']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer.tokenize(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', 'readers', '.', 'I', 'hope', 'you', 'find', 'it', 'interesting', '.', 'Please', 'do', 'reply', '.']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(text, pattern='\\w+|\\$[\\d\\.]+|\\S+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegularexpTokenizer uses the re.findall()function to perform tokenization\n",
    "by matching tokens. It uses the re.split() function to perform tokenization by\n",
    "matching gaps or spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how to tokenize using whitespaces: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'readers.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'it',\n",
       " 'interesting.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'reply.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer('\\s+',gaps=True)\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To select the words starting with a capital letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'She']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capt = RegexpTokenizer('[A-Z]\\w+')\n",
    "capt.tokenize(text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tokenization of strings can be done using whitespace—tab, space, or newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'secured',\n",
       " '90.56',\n",
       " '%',\n",
       " 'in',\n",
       " 'class',\n",
       " 'X',\n",
       " '.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'a',\n",
       " 'meritorious',\n",
       " 'student']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhitespaceTokenizer().tokenize(text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  LineTokenizer works by tokenizing text into lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' She secured 90.56 % in class X \\n. She is a meritorious student']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlanklineTokenizer().tokenize(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' She secured 90.56 % in class X ', '. She is a meritorious student']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LineTokenizer(blanklines='keep').tokenize(text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaceTokenizer works similar to sent.split(''):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'She',\n",
       " 'secured',\n",
       " '90.56',\n",
       " '%',\n",
       " 'in',\n",
       " 'class',\n",
       " 'X',\n",
       " '\\n.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'a',\n",
       " 'meritorious',\n",
       " 'student']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SpaceTokenizer().tokenize(text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk.tokenize.util module works by returning the sequence of tuples that are offsets of the tokens in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4),\n",
       " (5, 12),\n",
       " (13, 18),\n",
       " (19, 20),\n",
       " (21, 23),\n",
       " (24, 29),\n",
       " (30, 31),\n",
       " (33, 34),\n",
       " (35, 38),\n",
       " (39, 41),\n",
       " (42, 43),\n",
       " (44, 55),\n",
       " (56, 63)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(WhitespaceTokenizer().span_tokenize(text_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a sequence of spans, the sequence of relative spans can be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3),\n",
       " (1, 7),\n",
       " (1, 5),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 5),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 11),\n",
       " (1, 7)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(spans_to_relative(WhitespaceTokenizer().span_tokenize(text_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
