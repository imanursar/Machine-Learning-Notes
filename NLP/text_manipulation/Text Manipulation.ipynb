{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single packet text manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Removing html tags\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    document = TAG_RE.sub('', document)\n",
    "    \n",
    "    # removing all non-word characters such as special characters, numbers, etc.\n",
    "    # Removing Non-Word Characters, uppercase\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]), flags=re.I)\n",
    "    #Removing Digits from a String\n",
    "    document = re.sub(r'\\d', ' ', str(X[sen]))\n",
    "    #Removing Grouping Multiple Patterns, uppercase\n",
    "    document = re.sub(r\"[,@\\'?\\.$%_*!^]\", \"\", str(X[sen]), flags=re.I)\n",
    "    #or use this\n",
    "    # Remove punctuations and numbers\n",
    "    document = re.sub('[^a-zA-Z]', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    #removes spaces from the beginning \n",
    "    document = re.sub(r\"^\\s+\", \"\", document)\n",
    "    \n",
    "    #removes spaces from at the end \n",
    "    document = re.sub(r\"\\s+$\", \"\", document)\n",
    "    \n",
    "    # remove all the single characters\n",
    "    # Sometimes removing punctuation marks, such as an apostrophe, results in a single character which has no meaning\n",
    "    # jacob's to jacob s\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    # multiple spaces appear between words as a result of removing words or punctuation\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "#     # Removing prefixed 'b'\n",
    "#     document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "#     # Remove single characters from the start\n",
    "#     document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    # reduce the word into dictionary root form\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "def preprocess_text(document):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(document))\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    # Removing prefixed 'b'\n",
    "    # if your text string is in bytes format a character b is appended with the string.\n",
    "    # The b prefix signifies a bytes string literal.\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    # Lemmatization\n",
    "    tokens = document.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    tokens = [word for word in tokens if len(word) > 3]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "data['comment_text'] = data['comment_text'].str.lower()\n",
    "data['comment_text'] = data['comment_text'].apply(cleanHtml)\n",
    "data['comment_text'] = data['comment_text'].apply(cleanPunc)\n",
    "data['comment_text'] = data['comment_text'].apply(keepAlpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python's NLTK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\imanursar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\imanursar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nick', 'likes', 'play', 'football', ',', 'however', 'fond', 'tennis', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python's Gensim Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nick likes play football, fond tennis.\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "filtered_sentence = remove_stopwords(text)\n",
    "\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the SpaCy Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U spacy\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sentence)\n",
    "    # Remove all the special characters\n",
    "    # remove all the non-word characters (letters and numbers) \n",
    "    # from a string and keep the remaining characters\n",
    "    sentence = re.sub(r'\\W', ' ', str(sentence))\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Single character removal\n",
    "    # Sometimes removing punctuation marks, such as an apostrophe, \n",
    "    # results in a single character which has no meaning\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # Remove single characters from the start\n",
    "    sentence = re.sub(r'\\^[a-zA-Z]\\s+', ' ', sentence)\n",
    "    # Remove enter type to space\n",
    "    sentence = sentence.replace(\"\\n\",\" \")\n",
    "    # Substituting multiple spaces with single space\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    #added\n",
    "    # Removing prefixed 'b'\n",
    "    # if your text string is in bytes format a character b is appended with the string.\n",
    "    # The b prefix signifies a bytes string literal.\n",
    "    sentence = re.sub(r'^b\\s+', '', sentence)\n",
    "    #removes spaces from at the end \n",
    "    sentence = re.sub(r\"\\s+$\", \"\", sentence)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # Lemmatization\n",
    "    # reduce the word into dictionary root form\n",
    "    sentence = sentence.split()\n",
    "    sentence = [stemmer.lemmatize(word) for word in sentence]\n",
    "    \n",
    "    #added\n",
    "    # stop word from NLTK\n",
    "    # tokens = [word for word in tokens if word not in en_stop]\n",
    "    # words with length less than 4, such as \"era\", have also been removed.\n",
    "    tokens = [word for word in tokens if len(word) > 3]\n",
    "    \n",
    "    sentence = ' '.join(sentence)\n",
    "\n",
    "    #remove stopwords from gensim\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    return sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
